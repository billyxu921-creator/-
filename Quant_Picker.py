#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡åŒ–é€‰è‚¡å™¨ (Quant Picker)
å®ç°ï¼šAkShareæŒ‡æ ‡åˆé€‰ â†’ èˆ†æƒ…ç¢°æ’ â†’ DeepSeekç»ˆæç­›é€‰ â†’ AIæ½œåŠ›è‚¡æ¨è

æµç¨‹:
1. AkShareæŒ‡æ ‡åˆé€‰ï¼ˆå¸‚å€¼ã€æ¶¨å¹…ã€æ¢æ‰‹ç‡ï¼‰
2. èˆ†æƒ…ç¢°æ’ï¼ˆå¾®åš+å°çº¢ä¹¦çƒ­ç‚¹åŒ¹é…ï¼‰
3. DeepSeekç»ˆæç­›é€‰ï¼ˆAIé€‰å‡ºTOP 3ï¼‰
4. ç”ŸæˆAIæ½œåŠ›è‚¡æ¨èæŠ¥å‘Š
"""

import akshare as ak
import pandas as pd
import numpy as np
import json
import requests
import re
from datetime import datetime
import warnings
import os

warnings.filterwarnings('ignore')


class QuantPicker:
    """é‡åŒ–é€‰è‚¡å™¨"""
    
    def __init__(self, api_key=None):
        """
        åˆå§‹åŒ–é€‰è‚¡å™¨
        
        å‚æ•°:
            api_key: DeepSeek APIå¯†é’¥
        """
        # ä»é…ç½®æ–‡ä»¶è¯»å–API Key
        if api_key is None:
            try:
                from config import DEEPSEEK_CONFIG
                self.api_key = DEEPSEEK_CONFIG['api_key']
                self.api_base = DEEPSEEK_CONFIG['api_base']
            except ImportError:
                print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œè¯·è®¾ç½®API Key")
                self.api_key = "YOUR_API_KEY"
                self.api_base = "https://api.deepseek.com/v1"
        else:
            self.api_key = api_key
            self.api_base = "https://api.deepseek.com/v1"
        
        # èˆ†æƒ…å…³é”®è¯é…ç½®
        self.sentiment_keywords = {
            'high_priority': ['é‡ç»„', 'å¹¶è´­', 'æ”¶è´­', 'å…¥è‚¡', 'åˆ©å¥½', 'æ¶¨åœ', 'çªç ´'],
            'medium_priority': ['å¢æŒ', 'å›è´­', 'ä¸šç»©', 'ç›ˆåˆ©', 'åˆ†çº¢'],
            'low_priority': ['å…³æ³¨', 'çœ‹å¥½', 'æ¨è']
        }
        
        # å†å²æ•°æ®ç›®å½•
        self.history_dir = "discovery_history"
        
        print("âœ“ é‡åŒ–é€‰è‚¡å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def step1_akshare_screening(self):
        """
        Step 1: AkShareæŒ‡æ ‡åˆé€‰
        
        ç­›é€‰æ ‡å‡†:
        - 60 < å¸‚å€¼ < 200äº¿
        - 2% < æ¶¨å¹… < 6%
        - æ¢æ‰‹ç‡ > 5%
        
        è¿”å›:
            DataFrame: åˆé€‰è‚¡ç¥¨åˆ—è¡¨
        """
        print("\n" + "=" * 60)
        print("Step 1: AkShareæŒ‡æ ‡åˆé€‰")
        print("=" * 60)
        
        try:
            # è·å–å…¨Aè‚¡å®æ—¶è¡Œæƒ…
            print("\næ­£åœ¨è·å–å…¨Aè‚¡å®æ—¶è¡Œæƒ…...")
            df = ak.stock_zh_a_spot_em()
            
            print(f"âœ“ è·å–æˆåŠŸï¼Œå…± {len(df)} åªè‚¡ç¥¨")
            
            # æ•°æ®é¢„å¤„ç†
            print("\næ•°æ®é¢„å¤„ç†...")
            
            # ç¡®ä¿æ•°å€¼åˆ—ä¸ºfloatç±»å‹
            numeric_columns = ['æœ€æ–°ä»·', 'æ¶¨è·Œå¹…', 'æ¢æ‰‹ç‡', 'æ€»å¸‚å€¼', 'æµé€šå¸‚å€¼']
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # è®¡ç®—å¸‚å€¼ï¼ˆäº¿å…ƒï¼‰
            if 'æ€»å¸‚å€¼' in df.columns:
                df['å¸‚å€¼_äº¿'] = df['æ€»å¸‚å€¼'] / 100000000
            elif 'æµé€šå¸‚å€¼' in df.columns:
                df['å¸‚å€¼_äº¿'] = df['æµé€šå¸‚å€¼'] / 100000000
            else:
                print("Ã— æœªæ‰¾åˆ°å¸‚å€¼å­—æ®µ")
                return pd.DataFrame()
            
            # ç­›é€‰æ¡ä»¶
            print("\nåº”ç”¨ç­›é€‰æ¡ä»¶:")
            print("  - 60 < å¸‚å€¼ < 200äº¿")
            print("  - 2% < æ¶¨å¹… < 6%")
            print("  - æ¢æ‰‹ç‡ > 5%")
            
            # åº”ç”¨ç­›é€‰
            mask = (
                (df['å¸‚å€¼_äº¿'] > 60) & 
                (df['å¸‚å€¼_äº¿'] < 200) &
                (df['æ¶¨è·Œå¹…'] > 2) & 
                (df['æ¶¨è·Œå¹…'] < 6) &
                (df['æ¢æ‰‹ç‡'] > 5)
            )
            
            df_filtered = df[mask].copy()
            
            # æŒ‰æ¶¨å¹…æ’åº
            df_filtered = df_filtered.sort_values('æ¶¨è·Œå¹…', ascending=False)
            
            # é‡ç½®ç´¢å¼•
            df_filtered = df_filtered.reset_index(drop=True)
            
            print(f"\nâœ“ åˆé€‰å®Œæˆï¼Œç­›é€‰å‡º {len(df_filtered)} åªè‚¡ç¥¨")
            
            if len(df_filtered) > 0:
                print("\nã€åˆé€‰è‚¡ç¥¨TOP 10ã€‘")
                print("-" * 60)
                for i, (_, row) in enumerate(df_filtered.head(10).iterrows(), 1):
                    print(f"{i:2d}. {row['åç§°']:8s} ({row['ä»£ç ']}) "
                          f"æ¶¨å¹…:{row['æ¶¨è·Œå¹…']:5.2f}% "
                          f"å¸‚å€¼:{row['å¸‚å€¼_äº¿']:6.1f}äº¿ "
                          f"æ¢æ‰‹:{row['æ¢æ‰‹ç‡']:5.2f}%")
            
            return df_filtered
        
        except Exception as e:
            print(f"\nÃ— AkShareæ•°æ®è·å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()
    
    def step2_sentiment_match(self, df_stocks):
        """
        Step 2: èˆ†æƒ…ç¢°æ’
        
        å°†åˆé€‰è‚¡ç¥¨ä¸å¾®åšã€å°çº¢ä¹¦çƒ­ç‚¹æ•°æ®åŒ¹é…
        
        å‚æ•°:
            df_stocks: åˆé€‰è‚¡ç¥¨DataFrame
            
        è¿”å›:
            DataFrame: æ·»åŠ èˆ†æƒ…è¯„åˆ†çš„è‚¡ç¥¨åˆ—è¡¨
        """
        print("\n" + "=" * 60)
        print("Step 2: èˆ†æƒ…ç¢°æ’ (Sentiment Match)")
        print("=" * 60)
        
        if df_stocks.empty:
            print("Ã— æ— åˆé€‰è‚¡ç¥¨ï¼Œè·³è¿‡èˆ†æƒ…ç¢°æ’")
            return df_stocks
        
        # åŠ è½½èˆ†æƒ…æ•°æ®
        sentiment_data = self._load_sentiment_data()
        
        if not sentiment_data:
            print("âš ï¸  æœªæ‰¾åˆ°èˆ†æƒ…æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†")
            df_stocks['èˆ†æƒ…è¯„åˆ†'] = 50
            df_stocks['èˆ†æƒ…æ¥æº'] = 'æ— '
            df_stocks['åŒ¹é…å…³é”®è¯'] = ''
            df_stocks['åšä¸»å½±å“åŠ›'] = 1
            return df_stocks
        
        print(f"\nâœ“ åŠ è½½èˆ†æƒ…æ•°æ®: {len(sentiment_data)} æ¡")
        
        # ä¸ºæ¯åªè‚¡ç¥¨è®¡ç®—èˆ†æƒ…è¯„åˆ†
        print("\næ­£åœ¨åŒ¹é…èˆ†æƒ…æ•°æ®...")
        
        sentiment_scores = []
        sentiment_sources = []
        matched_keywords = []
        influencer_weights = []
        
        for _, stock in df_stocks.iterrows():
            stock_name = stock['åç§°']
            stock_code = stock['ä»£ç ']
            
            # åŒ¹é…èˆ†æƒ…æ•°æ®
            score, source, keywords, weight = self._match_sentiment(
                stock_name, stock_code, sentiment_data
            )
            
            sentiment_scores.append(score)
            sentiment_sources.append(source)
            matched_keywords.append(keywords)
            influencer_weights.append(weight)
        
        # æ·»åŠ èˆ†æƒ…å­—æ®µ
        df_stocks['èˆ†æƒ…è¯„åˆ†'] = sentiment_scores
        df_stocks['èˆ†æƒ…æ¥æº'] = sentiment_sources
        df_stocks['åŒ¹é…å…³é”®è¯'] = matched_keywords
        df_stocks['åšä¸»å½±å“åŠ›'] = influencer_weights
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        # ç»¼åˆå¾—åˆ† = æ¶¨å¹…æƒé‡(30%) + æ¢æ‰‹ç‡æƒé‡(20%) + èˆ†æƒ…è¯„åˆ†(50%)
        df_stocks['ç»¼åˆå¾—åˆ†'] = (
            df_stocks['æ¶¨è·Œå¹…'] * 0.3 +
            df_stocks['æ¢æ‰‹ç‡'] * 0.2 +
            df_stocks['èˆ†æƒ…è¯„åˆ†'] * 0.5
        )
        
        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        df_stocks = df_stocks.sort_values('ç»¼åˆå¾—åˆ†', ascending=False)
        df_stocks = df_stocks.reset_index(drop=True)
        
        print(f"\nâœ“ èˆ†æƒ…ç¢°æ’å®Œæˆ")
        
        # æ˜¾ç¤ºèˆ†æƒ…åŒ¹é…ç»“æœ
        matched_count = len(df_stocks[df_stocks['èˆ†æƒ…è¯„åˆ†'] > 50])
        print(f"  åŒ¹é…åˆ°èˆ†æƒ…: {matched_count} åª")
        
        if matched_count > 0:
            print("\nã€èˆ†æƒ…åŒ¹é…TOP 5ã€‘")
            print("-" * 80)
            for i, (_, row) in enumerate(df_stocks[df_stocks['èˆ†æƒ…è¯„åˆ†'] > 50].head(5).iterrows(), 1):
                print(f"{i}. {row['åç§°']:8s} "
                      f"èˆ†æƒ…è¯„åˆ†:{row['èˆ†æƒ…è¯„åˆ†']:5.1f} "
                      f"æ¥æº:{row['èˆ†æƒ…æ¥æº']:10s} "
                      f"å…³é”®è¯:{row['åŒ¹é…å…³é”®è¯']:20s} "
                      f"å½±å“åŠ›:Ã—{row['åšä¸»å½±å“åŠ›']}")
        
        return df_stocks
    
    def _load_sentiment_data(self):
        """
        åŠ è½½èˆ†æƒ…æ•°æ®ï¼ˆå¾®åš+å°çº¢ä¹¦ï¼‰
        
        è¿”å›:
            list: èˆ†æƒ…æ•°æ®åˆ—è¡¨
        """
        sentiment_data = []
        
        # 1. åŠ è½½æœ€æ–°çš„discoveryæ•°æ®
        try:
            if os.path.exists(self.history_dir):
                files = os.listdir(self.history_dir)
                json_files = [f for f in files if f.startswith('discovery_') and f.endswith('.json')]
                
                if json_files:
                    # è·å–æœ€æ–°æ–‡ä»¶
                    latest_file = sorted(json_files)[-1]
                    filepath = os.path.join(self.history_dir, latest_file)
                    
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        texts = data.get('texts', [])
                        
                        for text in texts:
                            sentiment_data.append({
                                'text': text,
                                'source': 'å…¨ç½‘é›·è¾¾',
                                'followers': 10000,  # é»˜è®¤ç²‰ä¸æ•°
                                'platform': 'å°çº¢ä¹¦+å¾®åš'
                            })
                    
                    print(f"  âœ“ åŠ è½½å…¨ç½‘é›·è¾¾æ•°æ®: {len(texts)} æ¡")
        except Exception as e:
            print(f"  âš ï¸  åŠ è½½å…¨ç½‘é›·è¾¾æ•°æ®å¤±è´¥: {e}")
        
        # 2. åŠ è½½å¾®åšæ•°æ®
        try:
            csv_files = [f for f in os.listdir('.') if f.startswith('weibo_clean') and f.endswith('.csv')]
            
            if csv_files:
                latest_weibo = sorted(csv_files)[-1]
                df_weibo = pd.read_csv(latest_weibo, encoding='utf-8-sig')
                
                for _, row in df_weibo.iterrows():
                    sentiment_data.append({
                        'text': str(row.get('åšæ–‡å†…å®¹', '')),
                        'source': 'å¾®åš',
                        'followers': row.get('ç²‰ä¸æ•°', 1000),
                        'platform': 'å¾®åš'
                    })
                
                print(f"  âœ“ åŠ è½½å¾®åšæ•°æ®: {len(df_weibo)} æ¡")
        except Exception as e:
            print(f"  âš ï¸  åŠ è½½å¾®åšæ•°æ®å¤±è´¥: {e}")
        
        return sentiment_data
    
    def _match_sentiment(self, stock_name, stock_code, sentiment_data):
        """
        åŒ¹é…å•åªè‚¡ç¥¨çš„èˆ†æƒ…æ•°æ®
        
        å‚æ•°:
            stock_name: è‚¡ç¥¨åç§°
            stock_code: è‚¡ç¥¨ä»£ç 
            sentiment_data: èˆ†æƒ…æ•°æ®åˆ—è¡¨
            
        è¿”å›:
            tuple: (èˆ†æƒ…è¯„åˆ†, æ¥æº, åŒ¹é…å…³é”®è¯, åšä¸»å½±å“åŠ›æƒé‡)
        """
        base_score = 50  # åŸºç¡€åˆ†
        max_score = 0
        best_source = 'æ— '
        best_keywords = ''
        max_weight = 1
        
        for item in sentiment_data:
            text = item['text']
            source = item['source']
            followers = item.get('followers', 1000)
            
            # æ£€æŸ¥æ˜¯å¦æåŠè¯¥è‚¡ç¥¨
            if stock_name in text or stock_code in text:
                # è®¡ç®—å½±å“åŠ›æƒé‡
                if followers >= 1000000:  # 100ä¸‡+
                    weight = 10
                elif followers >= 100000:  # 10ä¸‡+
                    weight = 3
                else:
                    weight = 1
                
                # æ£€æŸ¥å…³é”®è¯
                score = base_score
                matched_kw = []
                
                # é«˜ä¼˜å…ˆçº§å…³é”®è¯ (+30åˆ†)
                for kw in self.sentiment_keywords['high_priority']:
                    if kw in text:
                        score += 30
                        matched_kw.append(kw)
                        break
                
                # ä¸­ä¼˜å…ˆçº§å…³é”®è¯ (+20åˆ†)
                if not matched_kw:
                    for kw in self.sentiment_keywords['medium_priority']:
                        if kw in text:
                            score += 20
                            matched_kw.append(kw)
                            break
                
                # ä½ä¼˜å…ˆçº§å…³é”®è¯ (+10åˆ†)
                if not matched_kw:
                    for kw in self.sentiment_keywords['low_priority']:
                        if kw in text:
                            score += 10
                            matched_kw.append(kw)
                            break
                
                # åº”ç”¨å½±å“åŠ›æƒé‡
                weighted_score = score * weight
                
                # ä¿ç•™æœ€é«˜åˆ†
                if weighted_score > max_score:
                    max_score = weighted_score
                    best_source = source
                    best_keywords = ','.join(matched_kw)
                    max_weight = weight
        
        # å½’ä¸€åŒ–åˆ°0-100
        final_score = min(100, max_score / 10)
        
        return final_score, best_source, best_keywords, max_weight

    
    def step3_deepseek_selection(self, df_stocks, top_n=10):
        """
        Step 3: DeepSeekç»ˆæç­›é€‰
        
        å°†ç»¼åˆå¾—åˆ†æœ€é«˜çš„å‰Nåªè‚¡ç¥¨å‘é€ç»™DeepSeek AIè¿›è¡Œç»ˆæç­›é€‰
        
        å‚æ•°:
            df_stocks: å¸¦èˆ†æƒ…è¯„åˆ†çš„è‚¡ç¥¨DataFrame
            top_n: å‘é€ç»™AIçš„è‚¡ç¥¨æ•°é‡
            
        è¿”å›:
            dict: AIæ¨èç»“æœ
        """
        print("\n" + "=" * 60)
        print("Step 3: DeepSeekç»ˆæç­›é€‰")
        print("=" * 60)
        
        if df_stocks.empty:
            print("Ã— æ— è‚¡ç¥¨æ•°æ®ï¼Œè·³è¿‡AIç­›é€‰")
            return None
        
        # é€‰å–TOP N
        top_stocks = df_stocks.head(top_n)
        
        print(f"\né€‰å–ç»¼åˆå¾—åˆ†TOP {len(top_stocks)} åªè‚¡ç¥¨è¿›è¡ŒAIåˆ†æ...")
        
        # è·å–è‚¡ç¥¨æ–°é—»
        print("\næ­£åœ¨è·å–è‚¡ç¥¨æ–°é—»...")
        stocks_with_news = self._fetch_stock_news(top_stocks)
        
        # æ„å»ºAIè¾“å…¥
        ai_input = self._build_ai_input(stocks_with_news)
        
        print(f"\nè¾“å…¥å­—æ•°: {len(ai_input)} å­—")
        
        # è°ƒç”¨DeepSeek API
        print("\næ­£åœ¨è°ƒç”¨DeepSeek APIè¿›è¡Œç»ˆæç­›é€‰...")
        
        ai_result = self._call_deepseek_for_selection(ai_input)
        
        if ai_result:
            print("âœ“ AIç­›é€‰å®Œæˆ")
            return ai_result
        else:
            print("Ã— AIç­›é€‰å¤±è´¥")
            return None
    
    def _fetch_stock_news(self, df_stocks):
        """
        è·å–è‚¡ç¥¨æœ€æ–°æ–°é—»
        
        å‚æ•°:
            df_stocks: è‚¡ç¥¨DataFrame
            
        è¿”å›:
            DataFrame: æ·»åŠ æ–°é—»å­—æ®µçš„è‚¡ç¥¨æ•°æ®
        """
        news_list = []
        
        for _, stock in df_stocks.iterrows():
            stock_code = stock['ä»£ç ']
            stock_name = stock['åç§°']
            
            try:
                # å°è¯•è·å–ä¸ªè‚¡æ–°é—»
                news_df = ak.stock_news_em(symbol=stock_code)
                
                if not news_df.empty:
                    # å–æœ€æ–°3æ¡æ–°é—»æ ‡é¢˜
                    latest_news = news_df.head(3)['æ–°é—»æ ‡é¢˜'].tolist()
                    news_summary = '; '.join(latest_news)
                else:
                    news_summary = 'æš‚æ— æœ€æ–°æ–°é—»'
            
            except:
                news_summary = 'æš‚æ— æœ€æ–°æ–°é—»'
            
            news_list.append(news_summary)
        
        df_stocks['æœ€æ–°æ–°é—»'] = news_list
        
        return df_stocks
    
    def _build_ai_input(self, df_stocks):
        """
        æ„å»ºAIè¾“å…¥æ–‡æœ¬
        
        å‚æ•°:
            df_stocks: è‚¡ç¥¨DataFrame
            
        è¿”å›:
            str: AIè¾“å…¥æ–‡æœ¬
        """
        lines = []
        
        lines.append("ä»¥ä¸‹æ˜¯ä»Šæ—¥ç­›é€‰å‡ºçš„æ½œåŠ›è‚¡ç¥¨åˆ—è¡¨ï¼š\n")
        
        for i, (_, stock) in enumerate(df_stocks.iterrows(), 1):
            lines.append(f"ã€è‚¡ç¥¨{i}ã€‘{stock['åç§°']} ({stock['ä»£ç ']})")
            lines.append(f"  æ¶¨è·Œå¹…: {stock['æ¶¨è·Œå¹…']:.2f}%")
            lines.append(f"  æœ€æ–°ä»·: {stock['æœ€æ–°ä»·']:.2f}å…ƒ")
            lines.append(f"  å¸‚å€¼: {stock['å¸‚å€¼_äº¿']:.1f}äº¿")
            lines.append(f"  æ¢æ‰‹ç‡: {stock['æ¢æ‰‹ç‡']:.2f}%")
            lines.append(f"  æˆäº¤é‡: {stock.get('æˆäº¤é‡', 0)}")
            lines.append(f"  èˆ†æƒ…è¯„åˆ†: {stock['èˆ†æƒ…è¯„åˆ†']:.1f}/100")
            lines.append(f"  èˆ†æƒ…æ¥æº: {stock['èˆ†æƒ…æ¥æº']}")
            
            if stock['åŒ¹é…å…³é”®è¯']:
                lines.append(f"  åŒ¹é…å…³é”®è¯: {stock['åŒ¹é…å…³é”®è¯']}")
            
            if stock['åšä¸»å½±å“åŠ›'] > 1:
                lines.append(f"  åšä¸»å½±å“åŠ›: Ã—{stock['åšä¸»å½±å“åŠ›']}")
            
            lines.append(f"  æœ€æ–°æ–°é—»: {stock['æœ€æ–°æ–°é—»']}")
            lines.append(f"  ç»¼åˆå¾—åˆ†: {stock['ç»¼åˆå¾—åˆ†']:.2f}")
            lines.append("")
        
        return "\n".join(lines)
    
    def _call_deepseek_for_selection(self, input_text):
        """
        è°ƒç”¨DeepSeek APIè¿›è¡Œè‚¡ç¥¨ç­›é€‰
        
        å‚æ•°:
            input_text: è¾“å…¥æ–‡æœ¬
            
        è¿”å›:
            dict: AIåˆ†æç»“æœ
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±åŸºé‡‘ç»ç†ï¼Œæ‹¥æœ‰20å¹´çš„Aè‚¡æŠ•èµ„ç»éªŒã€‚

è¯·ä»æä¾›çš„è‚¡ç¥¨åˆ—è¡¨ä¸­é€‰å‡º3åªä»Šæ—¥æœ€å…·æ½œåŠ›çš„è‚¡ç¥¨ã€‚

ã€é‡è¦è§„åˆ™ - é˜²æ­¢å¹»è§‰ã€‘:
1. ä½ åªèƒ½æ ¹æ®æä¾›çš„è‚¡ç¥¨æ•°æ®å›ç­”ï¼Œä¸å¾—æ¨èåˆ—è¡¨å¤–çš„è‚¡ç¥¨
2. æ‰€æœ‰è‚¡ä»·æ•°æ®å¿…é¡»ä»¥æä¾›çš„akshareæ•°æ®ä¸ºå‡†ï¼Œä¸å¾—ç¼–é€ 
3. å¦‚æœèˆ†æƒ…è¯„åˆ†ä¸º0æˆ–æ— æ•°æ®ï¼Œè¯·æ˜ç¡®è¯´æ˜'æ— èˆ†æƒ…æ•°æ®æ”¯æŒ'
4. ç¦æ­¢ç¼–é€ ä»»ä½•æ–°é—»ã€æ”¿ç­–æˆ–äº‹ä»¶
5. ä¸¥ç¦æ¨èä»»ä½•éAè‚¡å¸‚åœºçš„è‚¡ç¥¨ä»£ç 
6. ç»™å‡ºæ¨èç†ç”±æ—¶ï¼Œå¿…é¡»åŸºäºæä¾›çš„çœŸå®æ•°æ®ï¼ˆæ¶¨è·Œå¹…ã€æ¢æ‰‹ç‡ã€èˆ†æƒ…è¯„åˆ†ï¼‰
7. æ­¢ç›ˆä½è®¡ç®—å¿…é¡»åŸºäºå½“å‰ä»·æ ¼ï¼Œä½¿ç”¨å…¬å¼ï¼šå½“å‰ä»·æ ¼ Ã— (1 + åˆç†æ¶¨å¹…%)
8. å¦‚æœæ•°æ®ä¸è¶³ä»¥åšå‡ºåˆ¤æ–­ï¼Œè¯·åœ¨risk_warningä¸­è¯´æ˜'æ•°æ®ä¸è¶³'
9. ä¸è¦ç¼–é€ ä»»ä½•æŠ€æœ¯æŒ‡æ ‡æˆ–è´¢åŠ¡æ•°æ®
10. æ‰€æœ‰ç»“è®ºå¿…é¡»åŸºäºæä¾›çš„çœŸå®æ•°æ®

åˆ†æè¦æ±‚ï¼š
1. ç»¼åˆè€ƒè™‘æŠ€æœ¯é¢ï¼ˆæ¶¨å¹…ã€æ¢æ‰‹ç‡ã€æˆäº¤é‡ï¼‰å’Œèˆ†æƒ…é¢ï¼ˆç¤¾äº¤åª’ä½“çƒ­åº¦ã€å…³é”®è¯ï¼‰
2. æ‰¾å‡ºæŠ€æœ¯é¢å’Œèˆ†æƒ…é¢çš„ç»“åˆç‚¹ï¼ˆä¾‹å¦‚ï¼šæŠ€æœ¯çªç ´+èˆ†æƒ…å‚¬åŒ–ï¼‰
3. ç»™å‡ºå…·ä½“çš„æ¨èç†ç”±ï¼ˆå¿…é¡»åŸºäºæä¾›çš„çœŸå®æ•°æ®ï¼‰
4. é¢„æµ‹ä¸€ä¸ªçŸ­æœŸæ­¢ç›ˆä½ï¼ˆ1-3ä¸ªäº¤æ˜“æ—¥ï¼ŒåŸºäºå½“å‰ä»·æ ¼è®¡ç®—ï¼‰

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
{
    "recommendations": [
        {
            "rank": 1,
            "stock_name": "è‚¡ç¥¨åç§°",
            "stock_code": "è‚¡ç¥¨ä»£ç ï¼ˆå¿…é¡»æ˜¯æä¾›åˆ—è¡¨ä¸­çš„çœŸå®ä»£ç ï¼‰",
            "reason": "æ¨èç†ç”±ï¼ˆå¿…é¡»åŸºäºæä¾›çš„çœŸå®æ•°æ®ï¼‰",
            "technical_analysis": "æŠ€æœ¯é¢åˆ†æï¼ˆåŸºäºæä¾›çš„æ¶¨è·Œå¹…ã€æ¢æ‰‹ç‡æ•°æ®ï¼‰",
            "sentiment_analysis": "èˆ†æƒ…é¢åˆ†æï¼ˆåŸºäºæä¾›çš„èˆ†æƒ…è¯„åˆ†ï¼Œå¦‚æœä¸º0åˆ™è¯´æ˜æ— æ•°æ®ï¼‰",
            "synergy_point": "æŠ€æœ¯é¢å’Œèˆ†æƒ…é¢çš„ç»“åˆç‚¹",
            "target_price": "æ­¢ç›ˆä½ï¼ˆå…ƒï¼ŒåŸºäºå½“å‰ä»·æ ¼è®¡ç®—ï¼‰",
            "expected_return": "é¢„æœŸæ”¶ç›Šç‡ï¼ˆ%ï¼‰",
            "risk_warning": "é£é™©æç¤ºï¼ˆå¦‚æœæ•°æ®ä¸è¶³è¯·è¯´æ˜ï¼‰"
        }
    ],
    "market_view": "æ•´ä½“å¸‚åœºè§‚ç‚¹ï¼ˆåŸºäºæä¾›çš„æ•°æ®ï¼‰",
    "strategy_suggestion": "æ“ä½œç­–ç•¥å»ºè®®ï¼ˆä¿å®ˆå»ºè®®ï¼‰",
    "data_source_note": "æ‰€æœ‰æ•°æ®æ¥æºäºakshareå®æ—¶è¡Œæƒ…"
}

æ³¨æ„ï¼šåªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚ä¸¥ç¦ç¼–é€ ä»»ä½•æ•°æ®ã€‚"""
        
        url = f"{self.api_base}/chat/completions"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": input_text}
            ],
            "temperature": 0.3,
            "max_tokens": 2000
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            
            if 'choices' in result and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content']
                
                # å°è¯•è§£æJSON
                try:
                    json_match = re.search(r'\{[\s\S]*\}', content)
                    if json_match:
                        json_str = json_match.group()
                        ai_result = json.loads(json_str)
                        return ai_result
                    else:
                        print("  è­¦å‘Š: æ— æ³•ä»å“åº”ä¸­æå–JSON")
                        return None
                except json.JSONDecodeError:
                    print(f"  è­¦å‘Š: JSONè§£æå¤±è´¥")
                    print(f"  åŸå§‹å“åº”: {content[:200]}...")
                    return None
            
            return None
            
        except Exception as e:
            print(f"  APIè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def step4_generate_report(self, df_stocks, ai_result):
        """
        Step 4: ç”ŸæˆAIæ½œåŠ›è‚¡æ¨èæŠ¥å‘Š
        
        å‚æ•°:
            df_stocks: è‚¡ç¥¨DataFrame
            ai_result: AIåˆ†æç»“æœ
            
        è¿”å›:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        print("\n" + "=" * 60)
        print("Step 4: ç”ŸæˆAIæ½œåŠ›è‚¡æ¨èæŠ¥å‘Š")
        print("=" * 60)
        
        today = datetime.now().strftime('%Y%m%d')
        filename = f"AIæ½œåŠ›è‚¡æ¨è_{today}.md"
        
        report_lines = []
        
        # æ ‡é¢˜
        report_lines.append("# ğŸš€ AIå…¶ä»–ä¸»é¢˜æ½œåŠ›è‚¡æ¨è")
        report_lines.append("")
        report_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        report_lines.append("")
        report_lines.append("---")
        report_lines.append("")
        
        # ç­›é€‰æ¦‚å†µ
        report_lines.append("## ğŸ“Š ç­›é€‰æ¦‚å†µ")
        report_lines.append("")
        report_lines.append(f"- **åˆé€‰è‚¡ç¥¨æ•°**: {len(df_stocks)} åª")
        report_lines.append(f"- **èˆ†æƒ…åŒ¹é…æ•°**: {len(df_stocks[df_stocks['èˆ†æƒ…è¯„åˆ†'] > 50])} åª")
        report_lines.append(f"- **AIç»ˆé€‰æ•°**: 3 åª")
        report_lines.append("")
        
        # ç­›é€‰æ ‡å‡†
        report_lines.append("## ğŸ¯ ç­›é€‰æ ‡å‡†")
        report_lines.append("")
        report_lines.append("### Step 1: AkShareæŒ‡æ ‡åˆé€‰")
        report_lines.append("- 60 < å¸‚å€¼ < 200äº¿")
        report_lines.append("- 2% < æ¶¨å¹… < 6%")
        report_lines.append("- æ¢æ‰‹ç‡ > 5%")
        report_lines.append("")
        report_lines.append("### Step 2: èˆ†æƒ…ç¢°æ’")
        report_lines.append("- åŒ¹é…å¾®åšã€å°çº¢ä¹¦çƒ­ç‚¹æ•°æ®")
        report_lines.append("- ç™¾ä¸‡ç²‰ä¸åšä¸»æåŠ â†’ é«˜ä¼˜å…ˆçº§")
        report_lines.append("- å…³é”®è¯åŠ æƒ: é‡ç»„/åˆ©å¥½/æ¶¨åœç­‰")
        report_lines.append("")
        report_lines.append("### Step 3: DeepSeekç»ˆæç­›é€‰")
        report_lines.append("- AIåŸºé‡‘ç»ç†ä»TOP 10ä¸­é€‰å‡ºTOP 3")
        report_lines.append("- ç»¼åˆæŠ€æœ¯é¢å’Œèˆ†æƒ…é¢")
        report_lines.append("- é¢„æµ‹çŸ­æœŸæ­¢ç›ˆä½")
        report_lines.append("")
        
        # AIæ¨èç»“æœ
        if ai_result and 'recommendations' in ai_result:
            recommendations = ai_result['recommendations']
            
            report_lines.append("## ğŸ† AIæ¨èTOP 3")
            report_lines.append("")
            
            for rec in recommendations:
                rank = rec.get('rank', 0)
                stock_name = rec.get('stock_name', '')
                stock_code = rec.get('stock_code', '')
                reason = rec.get('reason', '')
                technical = rec.get('technical_analysis', '')
                sentiment = rec.get('sentiment_analysis', '')
                synergy = rec.get('synergy_point', '')
                target = rec.get('target_price', '')
                expected_return = rec.get('expected_return', '')
                risk = rec.get('risk_warning', '')
                
                report_lines.append(f"### {rank}. {stock_name} ({stock_code})")
                report_lines.append("")
                
                # è·å–å®æ—¶æ•°æ®
                stock_data = df_stocks[df_stocks['ä»£ç '] == stock_code]
                if not stock_data.empty:
                    stock = stock_data.iloc[0]
                    report_lines.append(f"**å®æ—¶æ•°æ®**:")
                    report_lines.append(f"- æœ€æ–°ä»·: {stock['æœ€æ–°ä»·']:.2f}å…ƒ")
                    report_lines.append(f"- æ¶¨è·Œå¹…: {stock['æ¶¨è·Œå¹…']:.2f}%")
                    report_lines.append(f"- æ¢æ‰‹ç‡: {stock['æ¢æ‰‹ç‡']:.2f}%")
                    report_lines.append(f"- å¸‚å€¼: {stock['å¸‚å€¼_äº¿']:.1f}äº¿")
                    report_lines.append(f"- èˆ†æƒ…è¯„åˆ†: {stock['èˆ†æƒ…è¯„åˆ†']:.1f}/100")
                    report_lines.append("")
                
                report_lines.append(f"**æ¨èç†ç”±**: {reason}")
                report_lines.append("")
                
                report_lines.append(f"**æŠ€æœ¯é¢åˆ†æ**:")
                report_lines.append(f"> {technical}")
                report_lines.append("")
                
                report_lines.append(f"**èˆ†æƒ…é¢åˆ†æ**:")
                report_lines.append(f"> {sentiment}")
                report_lines.append("")
                
                report_lines.append(f"**æŠ€æœ¯+èˆ†æƒ…ç»“åˆç‚¹**:")
                report_lines.append(f"> {synergy}")
                report_lines.append("")
                
                report_lines.append(f"**æ­¢ç›ˆä½**: {target}")
                report_lines.append(f"**é¢„æœŸæ”¶ç›Š**: {expected_return}")
                report_lines.append("")
                
                report_lines.append(f"**é£é™©æç¤º**: {risk}")
                report_lines.append("")
                report_lines.append("---")
                report_lines.append("")
            
            # å¸‚åœºè§‚ç‚¹
            market_view = ai_result.get('market_view', '')
            if market_view:
                report_lines.append("## ğŸ“ˆ æ•´ä½“å¸‚åœºè§‚ç‚¹")
                report_lines.append("")
                report_lines.append(f"> {market_view}")
                report_lines.append("")
            
            # æ“ä½œç­–ç•¥
            strategy = ai_result.get('strategy_suggestion', '')
            if strategy:
                report_lines.append("## ğŸ’¡ æ“ä½œç­–ç•¥å»ºè®®")
                report_lines.append("")
                report_lines.append(f"> {strategy}")
                report_lines.append("")
        
        else:
            report_lines.append("## âš ï¸  AIåˆ†æå¤±è´¥")
            report_lines.append("")
            report_lines.append("æœªèƒ½è·å–AIæ¨èç»“æœï¼Œè¯·æ£€æŸ¥APIè¿æ¥ã€‚")
            report_lines.append("")
        
        # å€™é€‰è‚¡ç¥¨åˆ—è¡¨
        report_lines.append("## ğŸ“‹ å€™é€‰è‚¡ç¥¨åˆ—è¡¨ (TOP 10)")
        report_lines.append("")
        
        for i, (_, stock) in enumerate(df_stocks.head(10).iterrows(), 1):
            report_lines.append(f"### {i}. {stock['åç§°']} ({stock['ä»£ç ']})")
            report_lines.append("")
            report_lines.append(f"- æ¶¨è·Œå¹…: {stock['æ¶¨è·Œå¹…']:.2f}%")
            report_lines.append(f"- æœ€æ–°ä»·: {stock['æœ€æ–°ä»·']:.2f}å…ƒ")
            report_lines.append(f"- å¸‚å€¼: {stock['å¸‚å€¼_äº¿']:.1f}äº¿")
            report_lines.append(f"- æ¢æ‰‹ç‡: {stock['æ¢æ‰‹ç‡']:.2f}%")
            report_lines.append(f"- èˆ†æƒ…è¯„åˆ†: {stock['èˆ†æƒ…è¯„åˆ†']:.1f}/100")
            report_lines.append(f"- ç»¼åˆå¾—åˆ†: {stock['ç»¼åˆå¾—åˆ†']:.2f}")
            
            if stock['åŒ¹é…å…³é”®è¯']:
                report_lines.append(f"- åŒ¹é…å…³é”®è¯: {stock['åŒ¹é…å…³é”®è¯']}")
            
            report_lines.append("")
        
        # å…è´£å£°æ˜
        report_lines.append("---")
        report_lines.append("")
        report_lines.append("## âš ï¸  å…è´£å£°æ˜")
        report_lines.append("")
        report_lines.append("1. æœ¬æŠ¥å‘Šç”±AIç®—æ³•è‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ")
        report_lines.append("2. è‚¡ç¥¨æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…")
        report_lines.append("3. è¯·ç»“åˆè‡ªèº«é£é™©æ‰¿å—èƒ½åŠ›åšå‡ºæŠ•èµ„å†³ç­–")
        report_lines.append("4. ä¸æ„æˆä»»ä½•æŠ•èµ„å»ºè®®")
        report_lines.append("")
        
        # å†™å…¥æ–‡ä»¶
        report_content = "\n".join(report_lines)
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {filename}")
        
        return filename
    
    def run(self):
        """
        è¿è¡Œå®Œæ•´çš„é‡åŒ–é€‰è‚¡æµç¨‹
        """
        print("=" * 60)
        print("é‡åŒ–é€‰è‚¡å™¨ (Quant Picker)")
        print("=" * 60)
        print()
        
        # Step 1: AkShareæŒ‡æ ‡åˆé€‰
        df_stocks = self.step1_akshare_screening()
        
        if df_stocks.empty:
            print("\nÃ— åˆé€‰æ— ç»“æœï¼Œç¨‹åºç»“æŸ")
            return
        
        # Step 2: èˆ†æƒ…ç¢°æ’
        df_stocks = self.step2_sentiment_match(df_stocks)
        
        # Step 3: DeepSeekç»ˆæç­›é€‰
        ai_result = self.step3_deepseek_selection(df_stocks, top_n=10)
        
        # Step 4: ç”ŸæˆæŠ¥å‘Š
        report_file = self.step4_generate_report(df_stocks, ai_result)
        
        # ä¿å­˜å€™é€‰è‚¡ç¥¨æ•°æ®
        csv_file = f"quant_picker_candidates_{datetime.now().strftime('%Y%m%d')}.csv"
        df_stocks.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        print("\n" + "=" * 60)
        print("âœ… é‡åŒ–é€‰è‚¡å®Œæˆï¼")
        print("=" * 60)
        print(f"\nç”Ÿæˆæ–‡ä»¶:")
        print(f"  - AIæ¨èæŠ¥å‘Š: {report_file}")
        print(f"  - å€™é€‰è‚¡ç¥¨æ•°æ®: {csv_file}")
        
        # æ˜¾ç¤ºAIæ¨èç»“æœ
        if ai_result and 'recommendations' in ai_result:
            print("\nã€AIæ¨èTOP 3ã€‘")
            print("-" * 60)
            for rec in ai_result['recommendations']:
                print(f"{rec['rank']}. {rec['stock_name']} ({rec['stock_code']})")
                print(f"   æ­¢ç›ˆä½: {rec['target_price']} | é¢„æœŸæ”¶ç›Š: {rec['expected_return']}")
                print(f"   ç†ç”±: {rec['reason'][:50]}...")
                print()


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºé€‰è‚¡å™¨å®ä¾‹
    picker = QuantPicker()
    
    # è¿è¡Œé€‰è‚¡æµç¨‹
    picker.run()


if __name__ == "__main__":
    main()
