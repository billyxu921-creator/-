#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨ç½‘çƒ­ç‚¹å‘ç°å¼•æ“ (Discovery Engine)
è‡ªåŠ¨è¯†åˆ«å°çº¢ä¹¦å’Œå¾®åšä¸Šè®¨è®ºåº¦å¼‚å¸¸å‡é«˜çš„è‚¡ç¥¨æ¿å—

åŠŸèƒ½ç‰¹ç‚¹:
1. å¤šæºæ¢æµ‹: å°çº¢ä¹¦è´¢ç»é¢‘é“æ¨èæµ + å¾®åšè´¢ç»çƒ­æœæ¦œ + è‚¡ç¥¨è¶…è¯
2. å¼ºåˆ¶åçˆ¬: headless=Falseã€éšæœºç­‰å¾…5.5-12.2ç§’ã€æ‹Ÿäººæ»šåŠ¨ã€çœŸå®User-Agent
3. AIæ™ºèƒ½å‘ç°: å¯¹æ¯”ä»Šæ—¥ä¸æ˜¨æ—¥è¯é¢‘ï¼Œè¯†åˆ«åŠ¨é‡æœ€å¤§çš„3ä¸ªéé¢„è®¾æ¿å—
4. ç”ŸæˆMarkdownç®€æŠ¥: ã€å…¨ç½‘é›·è¾¾ï¼šä½ å¯èƒ½é”™è¿‡çš„çƒ­é—¨æœºä¼šã€‘
"""

import pandas as pd
import random
import time
import json
import requests
import re
import os
from datetime import datetime, timedelta
from playwright.sync_api import sync_playwright
from collections import Counter
import winsound  # Windowsç³»ç»Ÿèœ‚é¸£å£°ï¼ˆmacOSéœ€è¦æ›¿æ¢ä¸ºå…¶ä»–æ–¹æ¡ˆï¼‰


class DiscoveryEngine:
    """å…¨ç½‘çƒ­ç‚¹å‘ç°å¼•æ“"""
    
    def __init__(self, api_key=None):
        """
        åˆå§‹åŒ–å‘ç°å¼•æ“
        
        å‚æ•°:
            api_key: DeepSeek APIå¯†é’¥
        """
        # ä»é…ç½®æ–‡ä»¶è¯»å–API Key
        if api_key is None:
            try:
                from config import DEEPSEEK_CONFIG
                self.api_key = DEEPSEEK_CONFIG['api_key']
                self.api_base = DEEPSEEK_CONFIG['api_base']
            except ImportError:
                print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œè¯·è®¾ç½®API Key")
                self.api_key = "YOUR_API_KEY"
                self.api_base = "https://api.deepseek.com/v1"
        else:
            self.api_key = api_key
            self.api_base = "https://api.deepseek.com/v1"
        
        # çœŸå®çš„User-Agentåˆ—è¡¨ï¼ˆç”¨äºéšæœºåˆ‡æ¢ï¼‰
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
        ]
        
        # å†å²æ•°æ®å­˜å‚¨è·¯å¾„
        self.history_dir = "discovery_history"
        if not os.path.exists(self.history_dir):
            os.makedirs(self.history_dir)
        
        # AIåˆ†æçš„ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é‡‘èå¸‚åœºåˆ†æå¸ˆå’Œæ•°æ®ç§‘å­¦å®¶ã€‚

ä»»åŠ¡: åˆ†æä»Šæ—¥å’Œæ˜¨æ—¥çš„è´¢ç»è®¨è®ºæ–‡æœ¬ï¼Œè¯†åˆ«å‡ºè®¨è®ºåº¦å¼‚å¸¸å‡é«˜çš„è‚¡ç¥¨æ¿å—ã€‚

ã€é‡è¦è§„åˆ™ - é˜²æ­¢å¹»è§‰ã€‘:
1. ä½ åªèƒ½æ ¹æ®æä¾›çš„ä»Šæ—¥å’Œæ˜¨æ—¥æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†æ
2. å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰ç›¸å…³æ¿å—ä¿¡æ¯ï¼Œè¯·åœ¨reasonä¸­è¯´æ˜'æœªæ‰¾åˆ°ç›¸å…³å†…å®¹'
3. ç¦æ­¢ç¼–é€ ä»»ä½•æ–°é—»ã€æ”¿ç­–æˆ–äº‹ä»¶
4. å¢é•¿ç‡å¿…é¡»åŸºäºæä¾›çš„ä»Šæ—¥å’Œæ˜¨æ—¥æåŠæ¬¡æ•°è®¡ç®—ï¼Œä¸å¾—ç¼–é€ 
5. å¦‚æœæŸä¸ªæ¿å—æ˜¨æ—¥æåŠæ¬¡æ•°ä¸º0ï¼Œè¯·è°¨æ…åˆ¤æ–­æ˜¯å¦ä¸ºçœŸå®çƒ­ç‚¹
6. ä¸¥ç¦æ¨èä»»ä½•éAè‚¡å¸‚åœºçš„æ¿å—
7. ç«çˆ†åŸå› å¿…é¡»åŸºäºæä¾›çš„æ–‡æœ¬å†…å®¹ï¼Œä¸å¾—ç¼–é€ å…·ä½“äº‹ä»¶
8. å¦‚æœæ— æ³•ç¡®å®šåŸå› ï¼Œè¯·è¯´æ˜'åŸå› ä¸æ˜ï¼Œéœ€è¿›ä¸€æ­¥è§‚å¯Ÿ'
9. å…³é”®äº‹ä»¶å¿…é¡»ä»æä¾›çš„æ–‡æœ¬ä¸­æå–ï¼Œä¸å¾—ç¼–é€ 
10. æ‰€æœ‰ç»“è®ºå¿…é¡»åŸºäºæä¾›çš„çœŸå®æ•°æ®

åˆ†æè¦æ±‚:
1. æå–æ‰€æœ‰æåˆ°çš„è¡Œä¸šæ¿å—å…³é”®è¯ï¼ˆå¦‚ï¼šç…¤ç‚­ã€æ ¸ç”µã€æ–°èƒ½æºã€åŠå¯¼ä½“ã€åŒ»è¯ç­‰ï¼‰
2. å¯¹æ¯”ä»Šæ—¥ä¸æ˜¨æ—¥çš„è¯é¢‘å˜åŒ–ï¼Œè®¡ç®—å¢é•¿ç‡ï¼ˆå…¬å¼ï¼š(ä»Šæ—¥-æ˜¨æ—¥)/æ˜¨æ—¥Ã—100%ï¼‰
3. è¯†åˆ«å‡ºåŠ¨é‡æœ€å¤§çš„3ä¸ªéé¢„è®¾æ¿å—ï¼ˆæ’é™¤ï¼šé»„é‡‘ã€å¤§ç›˜ã€Aè‚¡ç­‰æ³›æŒ‡è¯ï¼‰
4. åˆ†ææ¯ä¸ªæ¿å—ç«çˆ†çš„åŸå› ï¼ˆå¿…é¡»åŸºäºæä¾›çš„æ–‡æœ¬å†…å®¹ï¼Œä¸å¾—ç¼–é€ ï¼‰

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰:
{
    "hot_sectors": [
        {
            "sector_name": "æ¿å—åç§°ï¼ˆå¿…é¡»åœ¨æä¾›çš„æ–‡æœ¬ä¸­å‡ºç°ï¼‰",
            "growth_rate": å¢é•¿ç‡ï¼ˆåŸºäºæä¾›çš„æ•°æ®è®¡ç®—ï¼Œå¦‚150è¡¨ç¤ºå¢é•¿150%ï¼‰,
            "today_mentions": ä»Šæ—¥æåŠæ¬¡æ•°ï¼ˆå¿…é¡»æ˜¯çœŸå®ç»Ÿè®¡å€¼ï¼‰,
            "yesterday_mentions": æ˜¨æ—¥æåŠæ¬¡æ•°ï¼ˆå¿…é¡»æ˜¯çœŸå®ç»Ÿè®¡å€¼ï¼‰,
            "reason": "ç«çˆ†åŸå› åˆ†æï¼ˆå¿…é¡»åŸºäºæä¾›çš„æ–‡æœ¬å†…å®¹ï¼Œå¦‚æœä¸ç¡®å®šåˆ™è¯´æ˜'åŸå› ä¸æ˜'ï¼‰",
            "confidence": ç½®ä¿¡åº¦(0.0-1.0ï¼Œå¦‚æœæ•°æ®ä¸è¶³è¯·é™ä½ç½®ä¿¡åº¦)
        }
    ],
    "market_sentiment": "æ•´ä½“å¸‚åœºæƒ…ç»ªæè¿°ï¼ˆåŸºäºæä¾›çš„æ–‡æœ¬ï¼‰",
    "key_events": ["å…³é”®äº‹ä»¶1ï¼ˆä»æ–‡æœ¬æå–ï¼‰", "å…³é”®äº‹ä»¶2ï¼ˆä»æ–‡æœ¬æå–ï¼‰", "å…³é”®äº‹ä»¶3ï¼ˆä»æ–‡æœ¬æå–ï¼‰"],
    "data_quality_note": "å¦‚æœæ•°æ®è´¨é‡å·®æˆ–ä¸è¶³ï¼Œè¯·åœ¨æ­¤è¯´æ˜"
}

æ³¨æ„: åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚ä¸¥ç¦ç¼–é€ ä»»ä½•æ•°æ®æˆ–äº‹ä»¶ã€‚"""

    
    def beep_alert(self):
        """
        éªŒè¯ç æç¤ºèœ‚é¸£å£°
        macOSç³»ç»Ÿä½¿ç”¨printæç¤ºï¼ˆå› ä¸ºwinsoundä»…æ”¯æŒWindowsï¼‰
        """
        try:
            # macOSç³»ç»Ÿä½¿ç”¨ç³»ç»Ÿæç¤ºéŸ³
            os.system('afplay /System/Library/Sounds/Glass.aiff')
        except:
            # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨printæç¤º
            print("\n" + "ğŸ””" * 20)
            print("âš ï¸  æ£€æµ‹åˆ°éªŒè¯ç ï¼è¯·æ‰‹åŠ¨å¤„ç†ï¼")
            print("ğŸ””" * 20 + "\n")
    
    def random_wait(self, min_sec=5.5, max_sec=12.2):
        """
        éšæœºç­‰å¾…ï¼ˆåçˆ¬ç­–ç•¥ï¼‰
        
        å‚æ•°:
            min_sec: æœ€å°ç­‰å¾…ç§’æ•°
            max_sec: æœ€å¤§ç­‰å¾…ç§’æ•°
        """
        wait_time = random.uniform(min_sec, max_sec)
        print(f"  â³ éšæœºç­‰å¾… {wait_time:.1f} ç§’...")
        time.sleep(wait_time)
    
    def slow_scroll(self, page, distance=800):
        """
        æ‹ŸäººåŒ–ç¼“æ…¢æ»šåŠ¨
        
        å‚æ•°:
            page: Playwrighté¡µé¢å¯¹è±¡
            distance: æ»šåŠ¨è·ç¦»ï¼ˆåƒç´ ï¼‰
        """
        # åˆ†å¤šæ¬¡å°å¹…åº¦æ»šåŠ¨ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·
        steps = random.randint(3, 6)
        step_distance = distance // steps
        
        for i in range(steps):
            page.mouse.wheel(0, step_distance)
            time.sleep(random.uniform(0.3, 0.8))
    
    def scrape_xiaohongshu(self, target_count=50, headless=False):
        """
        æŠ“å–å°çº¢ä¹¦è´¢ç»é¢‘é“æ¨èæµ
        
        å‚æ•°:
            target_count: ç›®æ ‡æŠ“å–ç¬”è®°æ•°é‡
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
            
        è¿”å›:
            list: ç¬”è®°æ•°æ®åˆ—è¡¨
        """
        print("=" * 60)
        print("å¼€å§‹æŠ“å–å°çº¢ä¹¦è´¢ç»é¢‘é“")
        print("=" * 60)
        
        all_notes = []
        
        try:
            with sync_playwright() as p:
                # éšæœºé€‰æ‹©User-Agent
                user_agent = random.choice(self.user_agents)
                
                print(f"å¯åŠ¨æµè§ˆå™¨ (headless={headless})...")
                browser = p.chromium.launch(headless=headless)
                
                context = browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent=user_agent
                )
                
                page = context.new_page()
                
                try:
                    # è®¿é—®å°çº¢ä¹¦è´¢ç»é¢‘é“
                    print("\næ­¥éª¤1: è®¿é—®å°çº¢ä¹¦è´¢ç»é¢‘é“...")
                    xhs_url = "https://www.xiaohongshu.com/explore"
                    page.goto(xhs_url, timeout=30000)
                    
                    self.random_wait(5.5, 8.0)
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
                    if self._check_login_required(page, "å°çº¢ä¹¦"):
                        print("\nâš ï¸  éœ€è¦ç™»å½•å°çº¢ä¹¦")
                        print("è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼ˆæ‰«ç æˆ–è´¦å·å¯†ç ï¼‰")
                        print("ç™»å½•å®Œæˆåï¼ŒæŒ‰å›è½¦ç»§ç»­...")
                        input()
                        
                        # ç™»å½•åé‡æ–°è®¿é—®
                        page.goto(xhs_url, timeout=30000)
                        self.random_wait(3.0, 5.0)
                    
                    # æœç´¢è´¢ç»ç›¸å…³å†…å®¹ï¼ˆåŒ…å«å¤šä¸ªçƒ­é—¨æ¿å—å…³é”®è¯ï¼‰
                    print("\næ­¥éª¤2: æœç´¢è´¢ç»ç›¸å…³å†…å®¹...")
                    try:
                        search_box = page.locator('input[placeholder*="æœç´¢"]').first
                        # ä½¿ç”¨æ›´å¹¿æ³›çš„æœç´¢å…³é”®è¯ï¼Œè¦†ç›–å¤šä¸ªçƒ­é—¨æ¿å—
                        search_keywords = "è´¢ç» ç†è´¢ è‚¡ç¥¨ æŠ•èµ„"
                        search_box.fill(search_keywords)
                        self.random_wait(1.0, 2.0)
                        search_box.press("Enter")
                        self.random_wait(5.5, 8.0)
                    except:
                        print("  âš ï¸  æœç´¢æ¡†å®šä½å¤±è´¥ï¼Œä½¿ç”¨æ¨èæµ...")
                    
                    # å¼€å§‹æ»šåŠ¨æŠ“å–æ¨èæµ
                    print(f"\næ­¥éª¤3: æ»šåŠ¨æŠ“å–æ¨èæµï¼ˆç›®æ ‡{target_count}æ¡ï¼‰...")
                    
                    scroll_count = 0
                    max_scrolls = 20  # æœ€å¤šæ»šåŠ¨20æ¬¡
                    
                    while len(all_notes) < target_count and scroll_count < max_scrolls:
                        scroll_count += 1
                        print(f"\n  ç¬¬ {scroll_count} æ¬¡æ»šåŠ¨ (å·²æŠ“å– {len(all_notes)}/{target_count})...")
                        
                        # æ£€æŸ¥éªŒè¯ç 
                        if self._check_captcha(page):
                            self.beep_alert()
                            print("  è¯·æ‰‹åŠ¨å®ŒæˆéªŒè¯ç ï¼Œå®ŒæˆåæŒ‰å›è½¦ç»§ç»­...")
                            input()
                        
                        # æå–å½“å‰é¡µé¢çš„ç¬”è®°
                        notes = self._extract_xiaohongshu_notes(page)
                        
                        # å»é‡æ·»åŠ 
                        for note in notes:
                            if note not in all_notes:
                                all_notes.append(note)
                        
                        print(f"  âœ“ æœ¬æ¬¡æå– {len(notes)} æ¡ï¼Œç´¯è®¡ {len(all_notes)} æ¡")
                        
                        # å¦‚æœå·²è¾¾åˆ°ç›®æ ‡ï¼Œåœæ­¢
                        if len(all_notes) >= target_count:
                            break
                        
                        # æ‹ŸäººåŒ–æ»šåŠ¨
                        self.slow_scroll(page, distance=random.randint(600, 1000))
                        
                        # éšæœºç­‰å¾…
                        self.random_wait(5.5, 12.2)
                    
                    print(f"\nâœ“ å°çº¢ä¹¦æŠ“å–å®Œæˆï¼Œå…±è·å– {len(all_notes)} æ¡ç¬”è®°")
                
                except Exception as e:
                    print(f"\nÃ— å°çº¢ä¹¦æŠ“å–å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                
                finally:
                    browser.close()
        
        except Exception as e:
            print(f"\nÃ— å°çº¢ä¹¦æ¨¡å—å¤±è´¥: {e}")
            print("  ç»§ç»­æ‰§è¡Œå…¶ä»–å¹³å°...")
        
        return all_notes

    
    def _extract_xiaohongshu_notes(self, page):
        """
        ä»å½“å‰é¡µé¢æå–å°çº¢ä¹¦ç¬”è®°
        
        å‚æ•°:
            page: Playwrighté¡µé¢å¯¹è±¡
            
        è¿”å›:
            list: ç¬”è®°æ•°æ®åˆ—è¡¨
        """
        notes = []
        
        try:
            # å°çº¢ä¹¦çš„ç¬”è®°å¡ç‰‡é€‰æ‹©å™¨ï¼ˆå¯èƒ½éœ€è¦æ ¹æ®å®é™…é¡µé¢è°ƒæ•´ï¼‰
            note_cards = page.locator('.note-item, .cover, section').all()
            
            for card in note_cards[:20]:  # æ¯æ¬¡æœ€å¤šæå–20æ¡
                try:
                    note_data = {}
                    
                    # æå–æ ‡é¢˜
                    try:
                        title = card.locator('.title, .note-title').inner_text()
                        note_data['æ ‡é¢˜'] = title.strip()
                    except:
                        note_data['æ ‡é¢˜'] = ''
                    
                    # æå–æ‘˜è¦/å†…å®¹
                    try:
                        content = card.locator('.desc, .content').inner_text()
                        note_data['å†…å®¹'] = content.strip()
                    except:
                        note_data['å†…å®¹'] = ''
                    
                    # æå–ç‚¹èµæ•°
                    try:
                        likes_text = card.locator('.like, .like-count').inner_text()
                        likes = re.findall(r'\d+', likes_text)
                        note_data['ç‚¹èµæ•°'] = int(likes[0]) if likes else 0
                    except:
                        note_data['ç‚¹èµæ•°'] = 0
                    
                    # æå–è¯„è®ºæ•°
                    try:
                        comments_text = card.locator('.comment, .comment-count').inner_text()
                        comments = re.findall(r'\d+', comments_text)
                        note_data['è¯„è®ºæ•°'] = int(comments[0]) if comments else 0
                    except:
                        note_data['è¯„è®ºæ•°'] = 0
                    
                    # æ¥æºå¹³å°
                    note_data['å¹³å°'] = 'å°çº¢ä¹¦'
                    note_data['æŠ“å–æ—¶é—´'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    
                    # åªä¿å­˜æœ‰æ ‡é¢˜æˆ–å†…å®¹çš„ç¬”è®°
                    if note_data['æ ‡é¢˜'] or note_data['å†…å®¹']:
                        notes.append(note_data)
                
                except:
                    continue
        
        except Exception as e:
            print(f"    æå–ç¬”è®°å¤±è´¥: {e}")
        
        return notes
    
    def scrape_weibo(self, headless=False):
        """
        æŠ“å–å¾®åšè´¢ç»çƒ­æœæ¦œå’Œè‚¡ç¥¨è¶…è¯
        
        å‚æ•°:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
            
        è¿”å›:
            list: å¾®åšæ•°æ®åˆ—è¡¨
        """
        print("\n" + "=" * 60)
        print("å¼€å§‹æŠ“å–å¾®åšè´¢ç»çƒ­æœå’Œè‚¡ç¥¨è¶…è¯")
        print("=" * 60)
        
        all_posts = []
        
        try:
            with sync_playwright() as p:
                user_agent = random.choice(self.user_agents)
                
                print(f"å¯åŠ¨æµè§ˆå™¨ (headless={headless})...")
                browser = p.chromium.launch(headless=headless)
                
                context = browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent=user_agent
                )
                
                page = context.new_page()
                
                try:
                    # 1. æŠ“å–è´¢ç»çƒ­æœæ¦œ
                    print("\næ­¥éª¤1: æŠ“å–å¾®åšè´¢ç»çƒ­æœæ¦œ...")
                    posts_hot = self._scrape_weibo_hot_search(page)
                    all_posts.extend(posts_hot)
                    print(f"  âœ“ è´¢ç»çƒ­æœ: {len(posts_hot)} æ¡")
                    
                    self.random_wait(5.5, 8.0)
                    
                    # 2. æŠ“å–è‚¡ç¥¨è¶…è¯
                    print("\næ­¥éª¤2: æŠ“å–è‚¡ç¥¨è¶…è¯...")
                    posts_topic = self._scrape_weibo_stock_topic(page)
                    all_posts.extend(posts_topic)
                    print(f"  âœ“ è‚¡ç¥¨è¶…è¯: {len(posts_topic)} æ¡")
                    
                    print(f"\nâœ“ å¾®åšæŠ“å–å®Œæˆï¼Œå…±è·å– {len(all_posts)} æ¡")
                
                except Exception as e:
                    print(f"\nÃ— å¾®åšæŠ“å–å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                
                finally:
                    browser.close()
        
        except Exception as e:
            print(f"\nÃ— å¾®åšæ¨¡å—å¤±è´¥: {e}")
            print("  ç»§ç»­æ‰§è¡Œå…¶ä»–å¹³å°...")
        
        return all_posts
    
    def _scrape_weibo_hot_search(self, page):
        """æŠ“å–å¾®åšè´¢ç»çƒ­æœæ¦œ"""
        posts = []
        
        try:
            # è®¿é—®å¾®åšçƒ­æœé¡µ
            weibo_hot_url = "https://s.weibo.com/top/summary"
            page.goto(weibo_hot_url, timeout=30000)
            
            self.random_wait(3.0, 5.0)
            
            # æ£€æŸ¥ç™»å½•
            if self._check_login_required(page, "å¾®åš"):
                print("\nâš ï¸  éœ€è¦ç™»å½•å¾®åš")
                print("è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•")
                print("ç™»å½•å®Œæˆåï¼ŒæŒ‰å›è½¦ç»§ç»­...")
                input()
                page.goto(weibo_hot_url, timeout=30000)
                self.random_wait(3.0, 5.0)
            
            # æŸ¥æ‰¾è´¢ç»ç›¸å…³çƒ­æœ
            hot_items = page.locator('tbody tr').all()
            
            for item in hot_items[:30]:  # å–å‰30æ¡
                try:
                    text = item.inner_text()
                    
                    # ç­›é€‰è´¢ç»ç›¸å…³ï¼ˆåŒ…å«ï¼šè‚¡ã€é‡‘èã€ç»æµã€Aè‚¡ã€æ¸¯è‚¡ç­‰å…³é”®è¯ï¼‰
                    # æ–°å¢ï¼šè‚¥æ–™ã€æˆ˜äº‰ã€å«æ˜Ÿã€è„‘æœºæ¥å£ç­‰çƒ­ç‚¹æ¿å—å…³é”®è¯
                    finance_keywords = [
                        # åŸºç¡€è´¢ç»å…³é”®è¯
                        'è‚¡', 'é‡‘è', 'ç»æµ', 'Aè‚¡', 'æ¸¯è‚¡', 'åŸºé‡‘', 'æŠ•èµ„', 'ç†è´¢', 'ä¸Šå¸‚', 'å¸‚å€¼',
                        # è¡Œä¸šæ¿å—å…³é”®è¯
                        'è‚¥æ–™', 'åŒ–è‚¥', 'ç£·è‚¥', 'é’¾è‚¥', 'æ°®è‚¥',  # è‚¥æ–™æ¿å—
                        'æˆ˜äº‰', 'å†›å·¥', 'å›½é˜²', 'æ­¦å™¨', 'å†›äº‹',  # æˆ˜äº‰/å†›å·¥æ¿å—
                        'å«æ˜Ÿ', 'èˆªå¤©', 'ç«ç®­', 'å¤ªç©º', 'åŒ—æ–—',  # å«æ˜Ÿ/èˆªå¤©æ¿å—
                        'è„‘æœºæ¥å£', 'è„‘æœº', 'ç¥ç»', 'é©¬æ–¯å…‹', 'Neuralink',  # è„‘æœºæ¥å£æ¿å—
                        # å…¶ä»–çƒ­é—¨æ¿å—
                        'ç…¤ç‚­', 'æ ¸ç”µ', 'æ–°èƒ½æº', 'å…‰ä¼', 'é£ç”µ', 'å‚¨èƒ½',
                        'åŠå¯¼ä½“', 'èŠ¯ç‰‡', 'äººå·¥æ™ºèƒ½', 'AI', 'æœºå™¨äºº',
                        'åŒ»è¯', 'ç”Ÿç‰©', 'ç–«è‹—', 'åŒ»ç–—',
                        'æˆ¿åœ°äº§', 'åœ°äº§', 'å»ºç­‘', 'åŸºå»º'
                    ]
                    
                    if any(keyword in text for keyword in finance_keywords):
                        post_data = {
                            'æ ‡é¢˜': text.strip(),
                            'å†…å®¹': text.strip(),
                            'ç‚¹èµæ•°': 0,
                            'è¯„è®ºæ•°': 0,
                            'å¹³å°': 'å¾®åšçƒ­æœ',
                            'æŠ“å–æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        }
                        posts.append(post_data)
                except:
                    continue
        
        except Exception as e:
            print(f"  è´¢ç»çƒ­æœæŠ“å–å¤±è´¥: {e}")
        
        return posts

    
    def _scrape_weibo_stock_topic(self, page):
        """æŠ“å–å¾®åšè‚¡ç¥¨è¶…è¯"""
        posts = []
        
        try:
            # è®¿é—®è‚¡ç¥¨è¶…è¯
            stock_topic_url = "https://s.weibo.com/weibo?q=%23è‚¡ç¥¨%23"
            page.goto(stock_topic_url, timeout=30000)
            
            self.random_wait(5.5, 8.0)
            
            # æ£€æŸ¥éªŒè¯ç 
            if self._check_captcha(page):
                self.beep_alert()
                print("  è¯·æ‰‹åŠ¨å®ŒæˆéªŒè¯ç ï¼Œå®ŒæˆåæŒ‰å›è½¦ç»§ç»­...")
                input()
            
            # æ»šåŠ¨åŠ è½½
            for i in range(3):
                self.slow_scroll(page, distance=800)
                self.random_wait(3.0, 5.0)
            
            # æå–å¾®åšå†…å®¹
            cards = page.locator('.card-wrap').all()
            
            for card in cards[:30]:
                try:
                    post_data = {}
                    
                    # æå–å†…å®¹
                    try:
                        content = card.locator('.txt').inner_text()
                        post_data['å†…å®¹'] = content.strip()
                    except:
                        post_data['å†…å®¹'] = ''
                    
                    # æå–æ ‡é¢˜ï¼ˆä½¿ç”¨å†…å®¹å‰50å­—ï¼‰
                    post_data['æ ‡é¢˜'] = post_data['å†…å®¹'][:50] + '...' if len(post_data['å†…å®¹']) > 50 else post_data['å†…å®¹']
                    
                    # æå–äº’åŠ¨æ•°æ®
                    try:
                        likes_text = card.locator('text=/èµ/').inner_text()
                        likes = re.findall(r'\d+', likes_text)
                        post_data['ç‚¹èµæ•°'] = int(likes[0]) if likes else 0
                    except:
                        post_data['ç‚¹èµæ•°'] = 0
                    
                    try:
                        comments_text = card.locator('text=/è¯„è®º/').inner_text()
                        comments = re.findall(r'\d+', comments_text)
                        post_data['è¯„è®ºæ•°'] = int(comments[0]) if comments else 0
                    except:
                        post_data['è¯„è®ºæ•°'] = 0
                    
                    post_data['å¹³å°'] = 'å¾®åšè¶…è¯'
                    post_data['æŠ“å–æ—¶é—´'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    
                    if post_data['å†…å®¹']:
                        posts.append(post_data)
                
                except:
                    continue
        
        except Exception as e:
            print(f"  è‚¡ç¥¨è¶…è¯æŠ“å–å¤±è´¥: {e}")
        
        return posts
    
    def _check_login_required(self, page, platform_name):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•"""
        try:
            # æ£€æŸ¥å¸¸è§çš„ç™»å½•æ ‡è¯†
            login_indicators = ['ç™»å½•', 'login', 'æ‰«ç ', 'è´¦å·']
            
            page_text = page.content().lower()
            
            for indicator in login_indicators:
                if indicator in page_text:
                    return True
            
            return False
        except:
            return False
    
    def _check_captcha(self, page):
        """æ£€æŸ¥æ˜¯å¦å‡ºç°éªŒè¯ç """
        try:
            # æ£€æŸ¥å¸¸è§çš„éªŒè¯ç æ ‡è¯†
            captcha_indicators = ['éªŒè¯', 'captcha', 'æ»‘å—', 'æ‹¼å›¾']
            
            page_text = page.content().lower()
            
            for indicator in captcha_indicators:
                if indicator in page_text:
                    return True
            
            return False
        except:
            return False
    
    def save_today_data(self, data_list):
        """
        ä¿å­˜ä»Šæ—¥æ•°æ®åˆ°å†å²æ–‡ä»¶
        
        å‚æ•°:
            data_list: æ•°æ®åˆ—è¡¨
        """
        if not data_list:
            return
        
        today = datetime.now().strftime('%Y%m%d')
        filename = os.path.join(self.history_dir, f"discovery_{today}.json")
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        all_text = []
        for item in data_list:
            text = f"{item.get('æ ‡é¢˜', '')} {item.get('å†…å®¹', '')}"
            all_text.append(text)
        
        # ä¿å­˜
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                'date': today,
                'count': len(data_list),
                'texts': all_text
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ“ ä»Šæ—¥æ•°æ®å·²ä¿å­˜: {filename}")
    
    def load_yesterday_data(self):
        """
        åŠ è½½æ˜¨æ—¥æ•°æ®
        
        è¿”å›:
            list: æ˜¨æ—¥æ–‡æœ¬åˆ—è¡¨
        """
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
        filename = os.path.join(self.history_dir, f"discovery_{yesterday}.json")
        
        if os.path.exists(filename):
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    print(f"âœ“ åŠ è½½æ˜¨æ—¥æ•°æ®: {data['count']} æ¡")
                    return data.get('texts', [])
            except:
                print("âš ï¸  æ˜¨æ—¥æ•°æ®åŠ è½½å¤±è´¥")
                return []
        else:
            print("âš ï¸  æœªæ‰¾åˆ°æ˜¨æ—¥æ•°æ®ï¼Œå°†ä½¿ç”¨ç©ºå¯¹æ¯”")
            return []

    
    def analyze_with_ai(self, today_texts, yesterday_texts):
        """
        ä½¿ç”¨DeepSeek AIåˆ†æçƒ­ç‚¹æ¿å—
        
        å‚æ•°:
            today_texts: ä»Šæ—¥æ–‡æœ¬åˆ—è¡¨
            yesterday_texts: æ˜¨æ—¥æ–‡æœ¬åˆ—è¡¨
            
        è¿”å›:
            dict: AIåˆ†æç»“æœ
        """
        print("\n" + "=" * 60)
        print("å¼€å§‹AIæ™ºèƒ½å‘ç°åˆ†æ")
        print("=" * 60)
        
        # åˆå¹¶æ–‡æœ¬
        today_combined = "\n".join(today_texts[:200])  # é™åˆ¶é•¿åº¦
        yesterday_combined = "\n".join(yesterday_texts[:200]) if yesterday_texts else "æ— æ˜¨æ—¥æ•°æ®"
        
        print(f"ä»Šæ—¥æ–‡æœ¬: {len(today_texts)} æ¡")
        print(f"æ˜¨æ—¥æ–‡æœ¬: {len(yesterday_texts)} æ¡")
        
        # æ„å»ºç”¨æˆ·è¾“å…¥
        user_input = f"""ä»Šæ—¥è®¨è®ºå†…å®¹ï¼ˆå…±{len(today_texts)}æ¡ï¼‰:
{today_combined}

---

æ˜¨æ—¥è®¨è®ºå†…å®¹ï¼ˆå…±{len(yesterday_texts)}æ¡ï¼‰:
{yesterday_combined}

è¯·åˆ†æä»Šæ—¥ç›¸æ¯”æ˜¨æ—¥ï¼Œå“ªäº›æ¿å—çš„è®¨è®ºåº¦å¼‚å¸¸å‡é«˜ã€‚"""
        
        # è°ƒç”¨DeepSeek API
        print("\næ­£åœ¨è°ƒç”¨DeepSeek APIè¿›è¡Œæ™ºèƒ½åˆ†æ...")
        
        result = self._call_deepseek_api(user_input)
        
        if result:
            print("âœ“ AIåˆ†æå®Œæˆ")
            return result
        else:
            print("Ã— AIåˆ†æå¤±è´¥")
            return None
    
    def _call_deepseek_api(self, user_input):
        """
        è°ƒç”¨DeepSeek API
        
        å‚æ•°:
            user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            
        è¿”å›:
            dict: åˆ†æç»“æœ
        """
        url = f"{self.api_base}/chat/completions"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_input}
            ],
            "temperature": 0.3,
            "max_tokens": 2000
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            
            if 'choices' in result and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content']
                
                # å°è¯•è§£æJSON
                try:
                    json_match = re.search(r'\{[\s\S]*\}', content)
                    if json_match:
                        json_str = json_match.group()
                        analysis_result = json.loads(json_str)
                        return analysis_result
                    else:
                        print("  è­¦å‘Š: æ— æ³•ä»å“åº”ä¸­æå–JSON")
                        return None
                except json.JSONDecodeError:
                    print(f"  è­¦å‘Š: JSONè§£æå¤±è´¥")
                    print(f"  åŸå§‹å“åº”: {content[:200]}...")
                    return None
            
            return None
            
        except Exception as e:
            print(f"  APIè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def generate_report(self, analysis_result, all_data):
        """
        ç”ŸæˆMarkdownç®€æŠ¥
        
        å‚æ•°:
            analysis_result: AIåˆ†æç»“æœ
            all_data: æ‰€æœ‰æŠ“å–çš„æ•°æ®
            
        è¿”å›:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        print("\n" + "=" * 60)
        print("ç”Ÿæˆå…¨ç½‘é›·è¾¾ç®€æŠ¥")
        print("=" * 60)
        
        today = datetime.now().strftime('%Y%m%d')
        filename = f"å…¨ç½‘é›·è¾¾æŠ¥å‘Š_{today}.md"
        
        report_lines = []
        
        # æ ‡é¢˜
        report_lines.append("# ã€å…¨ç½‘é›·è¾¾ï¼šä½ å¯èƒ½é”™è¿‡çš„çƒ­é—¨æœºä¼šã€‘")
        report_lines.append("")
        report_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        report_lines.append("")
        report_lines.append("---")
        report_lines.append("")
        
        # æ•°æ®æ¥æº
        report_lines.append("## ğŸ“¡ æ•°æ®æ¥æº")
        report_lines.append("")
        
        # ç»Ÿè®¡å„å¹³å°æ•°æ®é‡
        platform_stats = {}
        for item in all_data:
            platform = item.get('å¹³å°', 'æœªçŸ¥')
            platform_stats[platform] = platform_stats.get(platform, 0) + 1
        
        for platform, count in platform_stats.items():
            report_lines.append(f"- **{platform}**: {count} æ¡")
        
        report_lines.append(f"- **æ€»è®¡**: {len(all_data)} æ¡")
        report_lines.append("")
        
        # AIåˆ†æç»“æœ
        if analysis_result:
            # æ•´ä½“å¸‚åœºæƒ…ç»ª
            market_sentiment = analysis_result.get('market_sentiment', '')
            if market_sentiment:
                report_lines.append("## ğŸŒ¡ï¸ æ•´ä½“å¸‚åœºæƒ…ç»ª")
                report_lines.append("")
                report_lines.append(f"> {market_sentiment}")
                report_lines.append("")
            
            # çƒ­é—¨æ¿å—
            hot_sectors = analysis_result.get('hot_sectors', [])
            if hot_sectors:
                report_lines.append("## ğŸ”¥ è®¨è®ºåº¦å¼‚å¸¸å‡é«˜çš„æ¿å— TOP 3")
                report_lines.append("")
                
                for i, sector in enumerate(hot_sectors[:3], 1):
                    sector_name = sector.get('sector_name', 'æœªçŸ¥æ¿å—')
                    growth_rate = sector.get('growth_rate', 0)
                    today_mentions = sector.get('today_mentions', 0)
                    yesterday_mentions = sector.get('yesterday_mentions', 0)
                    reason = sector.get('reason', 'åŸå› æœªçŸ¥')
                    confidence = sector.get('confidence', 0)
                    
                    report_lines.append(f"### {i}. {sector_name} ğŸš€")
                    report_lines.append("")
                    report_lines.append(f"**å¢é•¿ç‡**: {growth_rate:.0f}%")
                    report_lines.append("")
                    report_lines.append(f"**è®¨è®ºçƒ­åº¦**:")
                    report_lines.append(f"- ä»Šæ—¥æåŠ: {today_mentions} æ¬¡")
                    report_lines.append(f"- æ˜¨æ—¥æåŠ: {yesterday_mentions} æ¬¡")
                    report_lines.append("")
                    report_lines.append(f"**ç«çˆ†åŸå› **: {reason}")
                    report_lines.append("")
                    report_lines.append(f"**ç½®ä¿¡åº¦**: {confidence:.0%}")
                    report_lines.append("")
                    
                    # çƒ­åº¦æ¡å½¢å›¾
                    bar_length = min(int(growth_rate / 10), 20)
                    bar = "ğŸŸ©" * bar_length
                    report_lines.append(f"```")
                    report_lines.append(f"çƒ­åº¦å¢é•¿: {bar}")
                    report_lines.append(f"```")
                    report_lines.append("")
            
            # å…³é”®äº‹ä»¶
            key_events = analysis_result.get('key_events', [])
            if key_events:
                report_lines.append("## ğŸ“° ä»Šæ—¥å…³é”®äº‹ä»¶")
                report_lines.append("")
                for i, event in enumerate(key_events, 1):
                    report_lines.append(f"{i}. {event}")
                report_lines.append("")
        
        # çƒ­é—¨å†…å®¹æ ·æœ¬
        report_lines.append("## ğŸ’¬ çƒ­é—¨å†…å®¹æ ·æœ¬")
        report_lines.append("")
        
        # æŒ‰ç‚¹èµæ•°æ’åº
        sorted_data = sorted(all_data, key=lambda x: x.get('ç‚¹èµæ•°', 0), reverse=True)
        
        for i, item in enumerate(sorted_data[:10], 1):
            platform = item.get('å¹³å°', 'æœªçŸ¥')
            title = item.get('æ ‡é¢˜', '')
            content = item.get('å†…å®¹', '')
            likes = item.get('ç‚¹èµæ•°', 0)
            comments = item.get('è¯„è®ºæ•°', 0)
            
            report_lines.append(f"### {i}. [{platform}]")
            report_lines.append("")
            
            if title:
                report_lines.append(f"**{title}**")
                report_lines.append("")
            
            if content:
                preview = content[:150] + '...' if len(content) > 150 else content
                report_lines.append(f"> {preview}")
                report_lines.append("")
            
            report_lines.append(f"ğŸ‘ {likes} | ğŸ’¬ {comments}")
            report_lines.append("")
        
        # å…è´£å£°æ˜
        report_lines.append("---")
        report_lines.append("")
        report_lines.append("## âš ï¸  ä½¿ç”¨è¯´æ˜")
        report_lines.append("")
        report_lines.append("1. æœ¬æŠ¥å‘ŠåŸºäºç¤¾äº¤åª’ä½“å…¬å¼€æ•°æ®å’ŒAIåˆ†æç”Ÿæˆ")
        report_lines.append("2. è®¨è®ºçƒ­åº¦ä¸ç­‰äºæŠ•èµ„ä»·å€¼ï¼Œè¯·ç†æ€§åˆ¤æ–­")
        report_lines.append("3. å»ºè®®ç»“åˆåŸºæœ¬é¢ã€æŠ€æœ¯é¢ç­‰å¤šç»´åº¦åˆ†æ")
        report_lines.append("4. ä¸æ„æˆä»»ä½•æŠ•èµ„å»ºè®®ï¼ŒæŠ•èµ„æœ‰é£é™©")
        report_lines.append("")
        
        # å†™å…¥æ–‡ä»¶
        report_content = "\n".join(report_lines)
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ“ ç®€æŠ¥å·²ä¿å­˜: {filename}")
        
        return filename

    
    def run(self, xhs_count=50, headless=False):
        """
        è¿è¡Œå®Œæ•´çš„å‘ç°æµç¨‹
        
        å‚æ•°:
            xhs_count: å°çº¢ä¹¦ç›®æ ‡æŠ“å–æ•°é‡
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
        """
        print("=" * 60)
        print("å…¨ç½‘çƒ­ç‚¹å‘ç°å¼•æ“ - Discovery Engine")
        print("=" * 60)
        print()
        
        all_data = []
        
        # 1. æŠ“å–å°çº¢ä¹¦
        print("\nã€ç¬¬1æ­¥ã€‘æŠ“å–å°çº¢ä¹¦è´¢ç»é¢‘é“...")
        try:
            xhs_data = self.scrape_xiaohongshu(target_count=xhs_count, headless=headless)
            all_data.extend(xhs_data)
            print(f"âœ“ å°çº¢ä¹¦: {len(xhs_data)} æ¡")
        except Exception as e:
            print(f"Ã— å°çº¢ä¹¦æŠ“å–å¤±è´¥: {e}")
            print("  ç»§ç»­æ‰§è¡Œ...")
        
        # 2. æŠ“å–å¾®åš
        print("\nã€ç¬¬2æ­¥ã€‘æŠ“å–å¾®åšè´¢ç»çƒ­æœå’Œè‚¡ç¥¨è¶…è¯...")
        try:
            weibo_data = self.scrape_weibo(headless=headless)
            all_data.extend(weibo_data)
            print(f"âœ“ å¾®åš: {len(weibo_data)} æ¡")
        except Exception as e:
            print(f"Ã— å¾®åšæŠ“å–å¤±è´¥: {e}")
            print("  ç»§ç»­æ‰§è¡Œ...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if not all_data:
            print("\nÃ— æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®ï¼Œç¨‹åºç»“æŸ")
            return
        
        print(f"\nâœ“ æ•°æ®æŠ“å–å®Œæˆï¼Œå…± {len(all_data)} æ¡")
        
        # ä¿å­˜åŸå§‹æ•°æ®
        today = datetime.now().strftime('%Y%m%d_%H%M%S')
        raw_filename = f"discovery_raw_{today}.csv"
        
        df = pd.DataFrame(all_data)
        df.to_csv(raw_filename, index=False, encoding='utf-8-sig')
        print(f"âœ“ åŸå§‹æ•°æ®å·²ä¿å­˜: {raw_filename}")
        
        # 3. ä¿å­˜ä»Šæ—¥æ•°æ®åˆ°å†å²
        print("\nã€ç¬¬3æ­¥ã€‘ä¿å­˜ä»Šæ—¥æ•°æ®...")
        self.save_today_data(all_data)
        
        # 4. åŠ è½½æ˜¨æ—¥æ•°æ®
        print("\nã€ç¬¬4æ­¥ã€‘åŠ è½½æ˜¨æ—¥æ•°æ®...")
        yesterday_texts = self.load_yesterday_data()
        
        # 5. AIåˆ†æ
        print("\nã€ç¬¬5æ­¥ã€‘AIæ™ºèƒ½å‘ç°åˆ†æ...")
        today_texts = [f"{item.get('æ ‡é¢˜', '')} {item.get('å†…å®¹', '')}" for item in all_data]
        
        analysis_result = self.analyze_with_ai(today_texts, yesterday_texts)
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        print("\nã€ç¬¬6æ­¥ã€‘ç”Ÿæˆå…¨ç½‘é›·è¾¾ç®€æŠ¥...")
        report_file = self.generate_report(analysis_result, all_data)
        
        # å®Œæˆ
        print("\n" + "=" * 60)
        print("âœ… å…¨ç½‘çƒ­ç‚¹å‘ç°å®Œæˆï¼")
        print("=" * 60)
        print(f"\nç”Ÿæˆæ–‡ä»¶:")
        print(f"  - åŸå§‹æ•°æ®: {raw_filename}")
        print(f"  - é›·è¾¾ç®€æŠ¥: {report_file}")
        print()


def main():
    """ä¸»å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                          â•‘
â•‘          å…¨ç½‘çƒ­ç‚¹å‘ç°å¼•æ“ - Discovery Engine             â•‘
â•‘                                                          â•‘
â•‘  åŠŸèƒ½: è‡ªåŠ¨è¯†åˆ«å°çº¢ä¹¦å’Œå¾®åšä¸Šè®¨è®ºåº¦å¼‚å¸¸å‡é«˜çš„æ¿å—       â•‘
â•‘                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # åˆ›å»ºå‘ç°å¼•æ“å®ä¾‹
    engine = DiscoveryEngine()
    
    # è¿è¡Œå‘ç°æµç¨‹
    # xhs_count=50: å°çº¢ä¹¦æŠ“å–50æ¡ç¬”è®°
    # headless=False: æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼Œæ–¹ä¾¿æ‰«ç ç™»å½•
    engine.run(xhs_count=50, headless=False)


if __name__ == "__main__":
    main()
