#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®åšé»„é‡‘æƒ…ç»ªè‡ªåŠ¨åˆ†æç³»ç»Ÿ - åŠ æƒä¼˜åŒ–ç‰ˆ
ä½¿ç”¨PlaywrightæŠ“å–å¾®åšæ•°æ®ï¼ŒDeepSeek AIåˆ†ææƒ…ç»ª
æ–°å¢ï¼šåšä¸»å½±å“åŠ›åŠ æƒã€å…³é”®è¯åŠ æˆã€åŠ æƒå…¬å¼è®¡ç®—
"""

import pandas as pd
import random
import time
import json
import requests
from datetime import datetime
from playwright.sync_api import sync_playwright
import re
import numpy as np


class WeiboSentimentWeightedAnalyzer:
    """å¾®åšæƒ…ç»ªåˆ†æå™¨ - åŠ æƒä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, api_key=None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        å‚æ•°:
            api_key: DeepSeek APIå¯†é’¥
        """
        # ä»é…ç½®æ–‡ä»¶è¯»å–API Key
        if api_key is None:
            try:
                from config import DEEPSEEK_CONFIG
                self.api_key = DEEPSEEK_CONFIG['api_key']
                self.api_base = DEEPSEEK_CONFIG['api_base']
            except ImportError:
                print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œè¯·è®¾ç½®API Key")
                self.api_key = "YOUR_API_KEY"
                self.api_base = "https://api.deepseek.com/v1"
        else:
            self.api_key = api_key
            self.api_base = "https://api.deepseek.com/v1"
        
        # è¥é”€å¹¿å‘Šå…³é”®è¯ï¼ˆç”¨äºè¿‡æ»¤ï¼‰
        self.spam_keywords = [
            'æŠ½å¥–', 'è½¬è¿ç ', 'ä»£è´­', 'å¾®å•†', 'åŠ å¾®ä¿¡', 'æ‰«ç ',
            'ä¼˜æƒ ', 'ä¿ƒé”€', 'æ‰“æŠ˜', 'ç§’æ€', 'æ‹¼å›¢', 'ç ä»·',
            'å…è´¹é¢†', 'é™æ—¶', 'ç‰¹ä»·', 'åŒ…é‚®', 'ç›´æ’­é—´'
        ]
        
        # å…³é”®è¯åŠ æˆé…ç½®
        self.boost_keywords = ['æ¶¨åœ', 'é‡ç»„', 'å…¥è‚¡', 'å¹¶è´­', 'æ”¶è´­', 'å¢æŒ', 'å›è´­']
        self.boost_ratio = 0.20  # 20%åŠ æˆ
        
        # AIåˆ†æçš„ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = """ä½ æ˜¯ä¸€ä½è¡Œä¸ºé‡‘èå­¦ä¸“å®¶ï¼Œè¯·åˆ†æè¿™äº›å¾®åšæ–‡æœ¬ï¼Œç»™å‡ºä»¥ä¸‹å†…å®¹ï¼š

ã€é‡è¦è§„åˆ™ - é˜²æ­¢å¹»è§‰ã€‘:
1. ä½ åªèƒ½æ ¹æ®æä¾›çš„å¾®åšå†…å®¹è¿›è¡Œåˆ†æ
2. å¦‚æœå¾®åšå†…å®¹ä¸è¶³ä»¥åˆ¤æ–­æƒ…ç»ªï¼Œè¯·åœ¨summaryä¸­è¯´æ˜'å†…å®¹ä¸è¶³ï¼Œæ— æ³•åˆ¤æ–­'
3. ç¦æ­¢ç¼–é€ ä»»ä½•å¾®åšå†…å®¹ã€æ–°é—»æˆ–äº‹ä»¶
4. é£é™©ç‚¹å’Œæœºä¼šç‚¹å¿…é¡»ä»å¾®åšå†…å®¹ä¸­æå–ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¯´æ˜'æœªæåŠ'
5. å¦‚æœå¾®åšä¸­æ²¡æœ‰æåˆ°å…·ä½“äº‹ä»¶ï¼Œä¸è¦ç¼–é€ äº‹ä»¶
6. æ—¥æœŸä¿¡æ¯å¿…é¡»æ¥è‡ªå¾®åšåŸæ–‡ï¼Œä¸å¾—ç¼–é€ 
7. æ‰€æœ‰ç»“è®ºå¿…é¡»åŸºäºæä¾›çš„çœŸå®å¾®åšå†…å®¹
8. å¦‚æœå¾®åšå†…å®¹ä¸»è¦æ˜¯å¹¿å‘Šæˆ–æ— å…³å†…å®¹ï¼Œè¯·åœ¨summaryä¸­è¯´æ˜

1. é»„é‡‘çœ‹æ¶¨æƒ…ç»ªæŒ‡æ•°ï¼ˆ0-100åˆ†ï¼‰- å¿…é¡»åŸºäºæä¾›çš„å¾®åšå†…å®¹
   - 0-20: æåº¦æ‚²è§‚ï¼Œææ…Œæ€§æŠ›å”®
   - 21-40: æ‚²è§‚ï¼Œçœ‹è·Œæƒ…ç»ªæ˜æ˜¾
   - 41-60: ä¸­æ€§ï¼Œè§‚æœ›ä¸ºä¸»
   - 61-80: ä¹è§‚ï¼Œçœ‹æ¶¨æƒ…ç»ªæ˜æ˜¾
   - 81-100: æåº¦ä¹è§‚ï¼Œè¿½æ¶¨çƒ­æƒ…é«˜

2. ä»Šæ—¥å¾®åšç”¨æˆ·æœ€æ‹…å¿ƒçš„3ä¸ªé£é™©ç‚¹ï¼ˆå¿…é¡»ä»å¾®åšå†…å®¹ä¸­æå–ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¯´æ˜'æœªæåŠ'ï¼‰

3. ä»Šæ—¥å¾®åšç”¨æˆ·æœ€æœŸå¾…çš„3ä¸ªæœºä¼šç‚¹ï¼ˆå¿…é¡»ä»å¾®åšå†…å®¹ä¸­æå–ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¯´æ˜'æœªæåŠ'ï¼‰

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
{
    "sentiment_index": æ•°å€¼(0-100),
    "sentiment_label": "æƒ…ç»ªæ ‡ç­¾",
    "risk_points": ["é£é™©ç‚¹1ï¼ˆä»å¾®åšæå–ï¼‰", "é£é™©ç‚¹2ï¼ˆä»å¾®åšæå–ï¼‰", "é£é™©ç‚¹3ï¼ˆä»å¾®åšæå–ï¼‰"],
    "opportunity_points": ["æœºä¼šç‚¹1ï¼ˆä»å¾®åšæå–ï¼‰", "æœºä¼šç‚¹2ï¼ˆä»å¾®åšæå–ï¼‰", "æœºä¼šç‚¹3ï¼ˆä»å¾®åšæå–ï¼‰"],
    "summary": "ä¸€å¥è¯æ€»ç»“ä»Šæ—¥é»„é‡‘æƒ…ç»ªï¼ˆåŸºäºæä¾›çš„å¾®åšå†…å®¹ï¼‰",
    "data_quality_note": "å¦‚æœæ•°æ®ä¸è¶³æˆ–è´¨é‡å·®ï¼Œè¯·åœ¨æ­¤è¯´æ˜"
}

æ³¨æ„ï¼šä¸¥ç¦ç¼–é€ ä»»ä½•å†…å®¹ï¼Œæ‰€æœ‰åˆ†æå¿…é¡»åŸºäºæä¾›çš„çœŸå®å¾®åšæ–‡æœ¬ã€‚"""
    
    def calculate_influence_weight(self, followers_count):
        """
        è®¡ç®—åšä¸»å½±å“åŠ›æƒé‡
        
        å‚æ•°:
            followers_count: ç²‰ä¸æ•°
            
        è¿”å›:
            float: å½±å“åŠ›æƒé‡
        """
        if followers_count >= 1000000:  # 100ä¸‡+
            return 10.0
        elif followers_count >= 100000:  # 10ä¸‡+
            return 3.0
        else:
            return 1.0
    
    def detect_keyword_boost(self, text):
        """
        æ£€æµ‹å…³é”®è¯åŠ æˆ
        
        å‚æ•°:
            text: æ–‡æœ¬å†…å®¹
            
        è¿”å›:
            tuple: (æ˜¯å¦æœ‰åŠ æˆ, åŒ¹é…çš„å…³é”®è¯åˆ—è¡¨)
        """
        matched_keywords = [kw for kw in self.boost_keywords if kw in text]
        has_boost = len(matched_keywords) > 0
        return has_boost, matched_keywords
    
    def calculate_weighted_sentiment(self, ai_score, has_boost, influence_weight):
        """
        è®¡ç®—åŠ æƒæƒ…ç»ªåˆ†æ•°
        
        å…¬å¼: Final_Score = (AI_Sentiment_Score + Keyword_Bonus) * Influence_Weight
        å½’ä¸€åŒ–: ç¡®ä¿æœ€ç»ˆåˆ†æ•°åœ¨0-100ä¹‹é—´
        
        å‚æ•°:
            ai_score: AIåŸå§‹åˆ†æ•° (0-100)
            has_boost: æ˜¯å¦æœ‰å…³é”®è¯åŠ æˆ
            influence_weight: å½±å“åŠ›æƒé‡
            
        è¿”å›:
            dict: åŒ…å«å„é¡¹åˆ†æ•°çš„å­—å…¸
        """
        # 1. è®¡ç®—å…³é”®è¯åŠ æˆ
        keyword_bonus = ai_score * self.boost_ratio if has_boost else 0
        
        # 2. åº”ç”¨åŠ æƒå…¬å¼
        weighted_score = (ai_score + keyword_bonus) * influence_weight
        
        # 3. å½’ä¸€åŒ–åˆ°0-100
        # æœ€å¤§å¯èƒ½å€¼: (100 + 20) * 10 = 1200
        # å½’ä¸€åŒ–å…¬å¼: score / max_possible * 100
        max_possible = (100 + 100 * self.boost_ratio) * 10  # 1200
        normalized_score = min(100, (weighted_score / max_possible) * 100)
        
        return {
            'ai_score': ai_score,
            'keyword_bonus': keyword_bonus,
            'influence_weight': influence_weight,
            'weighted_score': weighted_score,
            'final_score': round(normalized_score, 2)
        }

    
    def scrape_weibo_gold(self, pages=3, headless=False):
        """
        ä½¿ç”¨PlaywrightæŠ“å–å¾®åšé»„é‡‘ç›¸å…³å†…å®¹
        
        å‚æ•°:
            pages: æŠ“å–é¡µæ•°ï¼ˆé»˜è®¤3é¡µï¼‰
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆFalseæ–¹ä¾¿æ‰«ç ç™»å½•ï¼‰
            
        è¿”å›:
            DataFrame: åŒ…å«å¾®åšæ•°æ®
        """
        print("=" * 60)
        print("å¼€å§‹æŠ“å–å¾®åšé»„é‡‘ç›¸å…³å†…å®¹ï¼ˆåŠ æƒä¼˜åŒ–ç‰ˆï¼‰")
        print("=" * 60)
        
        all_posts = []
        
        with sync_playwright() as p:
            print(f"å¯åŠ¨æµè§ˆå™¨ (headless={headless})...")
            browser = p.chromium.launch(headless=headless)
            
            context = browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            )
            
            page = context.new_page()
            
            try:
                print("\næ­¥éª¤1: è®¿é—®å¾®åšæœç´¢é¡µ...")
                weibo_search_url = "https://s.weibo.com/weibo?q=é»„é‡‘"
                page.goto(weibo_search_url, timeout=30000)
                
                wait_time = random.uniform(3, 7)
                print(f"ç­‰å¾… {wait_time:.1f} ç§’...")
                time.sleep(wait_time)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
                if "login" in page.url.lower() or page.locator('text=ç™»å½•').count() > 0:
                    print("\nâš ï¸  éœ€è¦ç™»å½•å¾®åš")
                    print("è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼ˆæ‰«ç æˆ–è´¦å·å¯†ç ï¼‰")
                    print("ç™»å½•å®Œæˆåï¼ŒæŒ‰å›è½¦ç»§ç»­...")
                    input()
                    
                    page.goto(weibo_search_url, timeout=30000)
                    time.sleep(random.uniform(3, 5))
                
                print(f"\næ­¥éª¤2: å¼€å§‹æŠ“å–æ•°æ®ï¼ˆå…±{pages}é¡µï¼‰...")
                
                for page_num in range(1, pages + 1):
                    print(f"\næ­£åœ¨æŠ“å–ç¬¬ {page_num}/{pages} é¡µ...")
                    
                    try:
                        page.wait_for_selector('.card-wrap', timeout=10000)
                    except:
                        print("  âš ï¸  é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­...")
                    
                    # æ»šåŠ¨é¡µé¢
                    print("  æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹...")
                    for scroll in range(3):
                        page.evaluate('window.scrollBy(0, 800)')
                        time.sleep(random.uniform(1, 2))
                    
                    # æå–å½“å‰é¡µé¢çš„å¾®åšå†…å®¹ï¼ˆåŒ…å«ç²‰ä¸æ•°ï¼‰
                    posts = self._extract_posts_from_page(page)
                    all_posts.extend(posts)
                    print(f"  âœ“ æœ¬é¡µæå– {len(posts)} æ¡å¾®åš")
                    
                    # ç¿»é¡µ
                    if page_num < pages:
                        try:
                            next_button = page.locator('a.next')
                            if next_button.count() > 0:
                                print("  ç‚¹å‡»ä¸‹ä¸€é¡µ...")
                                next_button.click()
                                
                                wait_time = random.uniform(4, 8)
                                print(f"  ç­‰å¾… {wait_time:.1f} ç§’...")
                                time.sleep(wait_time)
                            else:
                                print("  æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œåœæ­¢æŠ“å–")
                                break
                        except Exception as e:
                            print(f"  ç¿»é¡µå¤±è´¥: {e}")
                            break
                
                print(f"\nâœ“ æŠ“å–å®Œæˆï¼Œå…±è·å– {len(all_posts)} æ¡å¾®åš")
                
            except Exception as e:
                print(f"\nÃ— æŠ“å–è¿‡ç¨‹å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
            
            finally:
                print("\nå…³é—­æµè§ˆå™¨...")
                browser.close()
        
        if all_posts:
            df = pd.DataFrame(all_posts)
            return df
        else:
            return pd.DataFrame()
    
    def _extract_posts_from_page(self, page):
        """
        ä»å½“å‰é¡µé¢æå–å¾®åšå†…å®¹ï¼ˆåŒ…å«ç²‰ä¸æ•°ï¼‰
        
        å‚æ•°:
            page: Playwrighté¡µé¢å¯¹è±¡
            
        è¿”å›:
            list: å¾®åšæ•°æ®åˆ—è¡¨
        """
        posts = []
        
        try:
            cards = page.locator('.card-wrap').all()
            
            for card in cards:
                try:
                    post_data = {}
                    
                    # æå–åšä¸»å
                    try:
                        author = card.locator('.name').inner_text()
                        post_data['åšä¸»å'] = author.strip()
                    except:
                        post_data['åšä¸»å'] = 'æœªçŸ¥'
                    
                    # æå–ç²‰ä¸æ•°ï¼ˆæ–°å¢ï¼‰
                    try:
                        # å°è¯•å¤šç§é€‰æ‹©å™¨
                        followers_text = ''
                        
                        # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«"ç²‰ä¸"çš„æ–‡æœ¬
                        followers_elem = card.locator('text=/ç²‰ä¸/')
                        if followers_elem.count() > 0:
                            followers_text = followers_elem.first.inner_text()
                        
                        # æ–¹æ³•2: æŸ¥æ‰¾infoåŒºåŸŸ
                        if not followers_text:
                            info_elem = card.locator('.info')
                            if info_elem.count() > 0:
                                info_text = info_elem.inner_text()
                                if 'ç²‰ä¸' in info_text:
                                    followers_text = info_text
                        
                        # è§£æç²‰ä¸æ•°
                        if followers_text:
                            # æå–æ•°å­—å’Œå•ä½ï¼ˆå¦‚ï¼š123ä¸‡ã€45.6ä¸‡ã€1234ï¼‰
                            match = re.search(r'(\d+\.?\d*)\s*([ä¸‡åƒç™¾]?)', followers_text)
                            if match:
                                num = float(match.group(1))
                                unit = match.group(2)
                                
                                if unit == 'ä¸‡':
                                    followers_count = int(num * 10000)
                                elif unit == 'åƒ':
                                    followers_count = int(num * 1000)
                                elif unit == 'ç™¾':
                                    followers_count = int(num * 100)
                                else:
                                    followers_count = int(num)
                                
                                post_data['ç²‰ä¸æ•°'] = followers_count
                            else:
                                post_data['ç²‰ä¸æ•°'] = 0
                        else:
                            post_data['ç²‰ä¸æ•°'] = 0
                    except:
                        post_data['ç²‰ä¸æ•°'] = 0
                    
                    # æå–åšæ–‡å†…å®¹
                    try:
                        content = card.locator('.txt').inner_text()
                        post_data['åšæ–‡å†…å®¹'] = content.strip()
                    except:
                        post_data['åšæ–‡å†…å®¹'] = ''
                    
                    # æå–å‘å¸ƒæ—¶é—´
                    try:
                        pub_time = card.locator('.from').inner_text()
                        post_data['å‘å¸ƒæ—¶é—´'] = pub_time.strip()
                    except:
                        post_data['å‘å¸ƒæ—¶é—´'] = ''
                    
                    # æå–ç‚¹èµæ•°
                    try:
                        likes = card.locator('text=/èµ/').inner_text()
                        like_num = re.findall(r'\d+', likes)
                        post_data['ç‚¹èµæ•°'] = int(like_num[0]) if like_num else 0
                    except:
                        post_data['ç‚¹èµæ•°'] = 0
                    
                    # æå–è½¬å‘æ•°
                    try:
                        retweets = card.locator('text=/è½¬å‘/').inner_text()
                        retweet_num = re.findall(r'\d+', retweets)
                        post_data['è½¬å‘æ•°'] = int(retweet_num[0]) if retweet_num else 0
                    except:
                        post_data['è½¬å‘æ•°'] = 0
                    
                    # è®¡ç®—å½±å“åŠ›æƒé‡ï¼ˆæ–°å¢ï¼‰
                    post_data['å½±å“åŠ›æƒé‡'] = self.calculate_influence_weight(post_data['ç²‰ä¸æ•°'])
                    
                    # æ£€æµ‹å…³é”®è¯åŠ æˆï¼ˆæ–°å¢ï¼‰
                    has_boost, matched_kw = self.detect_keyword_boost(post_data['åšæ–‡å†…å®¹'])
                    post_data['å…³é”®è¯åŠ æˆ'] = has_boost
                    post_data['åŒ¹é…å…³é”®è¯'] = ','.join(matched_kw) if matched_kw else ''
                    
                    if post_data['åšæ–‡å†…å®¹']:
                        posts.append(post_data)
                
                except:
                    continue
        
        except Exception as e:
            print(f"    æå–å¾®åšå¤±è´¥: {e}")
        
        return posts

    
    def clean_data(self, df):
        """
        æ¸…æ´—æ•°æ®
        
        å‚æ•°:
            df: åŸå§‹æ•°æ®DataFrame
            
        è¿”å›:
            DataFrame: æ¸…æ´—åçš„æ•°æ®
        """
        print("\n" + "=" * 60)
        print("å¼€å§‹æ•°æ®æ¸…æ´—")
        print("=" * 60)
        
        original_count = len(df)
        print(f"åŸå§‹æ•°æ®: {original_count} æ¡")
        
        # 1. è¿‡æ»¤è¥é”€å¹¿å‘Š
        print("\næ­¥éª¤1: è¿‡æ»¤è¥é”€å¹¿å‘Š...")
        mask = df['åšæ–‡å†…å®¹'].apply(lambda x: not any(keyword in str(x) for keyword in self.spam_keywords))
        df = df[mask].copy()
        print(f"  è¿‡æ»¤å: {len(df)} æ¡ (ç§»é™¤ {original_count - len(df)} æ¡)")
        
        # 2. å»é™¤è¿‡çŸ­çš„å†…å®¹
        print("\næ­¥éª¤2: è¿‡æ»¤è¿‡çŸ­å†…å®¹...")
        before = len(df)
        df = df[df['åšæ–‡å†…å®¹'].str.len() >= 10].copy()
        print(f"  è¿‡æ»¤å: {len(df)} æ¡ (ç§»é™¤ {before - len(df)} æ¡)")
        
        # 3. å»é‡
        print("\næ­¥éª¤3: å»é™¤é‡å¤å†…å®¹...")
        before = len(df)
        df = df.drop_duplicates(subset=['åšæ–‡å†…å®¹'], keep='first')
        print(f"  å»é‡å: {len(df)} æ¡ (ç§»é™¤ {before - len(df)} æ¡)")
        
        # 4. é‡ç½®ç´¢å¼•
        df = df.reset_index(drop=True)
        
        # 5. ç»Ÿè®¡åŠ æƒä¿¡æ¯
        print(f"\nâœ“ æ¸…æ´—å®Œæˆï¼Œæœ€ç»ˆä¿ç•™ {len(df)} æ¡æœ‰æ•ˆæ•°æ®")
        
        if not df.empty:
            print("\nã€åŠ æƒç»Ÿè®¡ã€‘")
            high_influence = len(df[df['å½±å“åŠ›æƒé‡'] == 10])
            mid_influence = len(df[df['å½±å“åŠ›æƒé‡'] == 3])
            low_influence = len(df[df['å½±å“åŠ›æƒé‡'] == 1])
            
            print(f"  é«˜å½±å“åŠ›åšä¸»(100ä¸‡+ç²‰ä¸): {high_influence} æ¡ (æƒé‡Ã—10)")
            print(f"  ä¸­å½±å“åŠ›åšä¸»(10ä¸‡+ç²‰ä¸): {mid_influence} æ¡ (æƒé‡Ã—3)")
            print(f"  æ™®é€šåšä¸»: {low_influence} æ¡ (æƒé‡Ã—1)")
            
            boost_count = len(df[df['å…³é”®è¯åŠ æˆ'] == True])
            print(f"  åŒ…å«å…³é”®è¯åŠ æˆ: {boost_count} æ¡ (+20%)")
        
        return df
    
    def analyze_sentiment_with_ai_weighted(self, df):
        """
        ä½¿ç”¨DeepSeek AIåˆ†ææƒ…ç»ªï¼ˆåŠ æƒç‰ˆæœ¬ï¼‰
        
        å‚æ•°:
            df: æ¸…æ´—åçš„æ•°æ®DataFrame
            
        è¿”å›:
            dict: AIåˆ†æç»“æœï¼ˆåŒ…å«åŠ æƒè®¡ç®—ï¼‰
        """
        print("\n" + "=" * 60)
        print("å¼€å§‹AIæƒ…ç»ªåˆ†æï¼ˆåŠ æƒä¼˜åŒ–ç‰ˆï¼‰")
        print("=" * 60)
        
        if df.empty:
            print("Ã— æ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æ")
            return None
        
        # 1. æŒ‰å½±å“åŠ›æƒé‡æ’åºï¼Œä¼˜å…ˆåˆ†æé«˜å½±å“åŠ›åšä¸»çš„å†…å®¹
        print(f"\nå‡†å¤‡åˆ†æ {len(df)} æ¡å¾®åš...")
        df_sorted = df.sort_values(by=['å½±å“åŠ›æƒé‡', 'ç‚¹èµæ•°', 'è½¬å‘æ•°'], ascending=False)
        
        # å–å‰50æ¡æˆ–å…¨éƒ¨
        top_posts = df_sorted.head(50)
        
        # åˆå¹¶å†…å®¹ï¼ˆæ ‡æ³¨å½±å“åŠ›å’Œå…³é”®è¯ï¼‰
        combined_text = "\n\n---\n\n".join([
            f"ã€å¾®åš{i+1}ã€‘(ç²‰ä¸:{row['ç²‰ä¸æ•°']}, æƒé‡:{row['å½±å“åŠ›æƒé‡']}, "
            f"å…³é”®è¯:{'æ˜¯' if row['å…³é”®è¯åŠ æˆ'] else 'å¦'})\n{row['åšæ–‡å†…å®¹']}"
            for i, (_, row) in enumerate(top_posts.iterrows())
        ])
        
        print(f"é€‰å– {len(top_posts)} æ¡ä»£è¡¨æ€§å¾®åšè¿›è¡Œåˆ†æ")
        print(f"æ€»å­—æ•°: {len(combined_text)} å­—")
        
        # 2. è°ƒç”¨DeepSeek APIè·å–åŸºç¡€æƒ…ç»ªåˆ†æ•°
        print("\næ­£åœ¨è°ƒç”¨DeepSeek API...")
        ai_result = self._call_deepseek_api(combined_text)
        
        if not ai_result:
            print("Ã— AIåˆ†æå¤±è´¥")
            return None
        
        print("âœ“ AIåŸºç¡€åˆ†æå®Œæˆ")
        
        # 3. è®¡ç®—åŠ æƒæƒ…ç»ªåˆ†æ•°
        print("\næ­£åœ¨è®¡ç®—åŠ æƒæƒ…ç»ªåˆ†æ•°...")
        
        ai_base_score = ai_result.get('sentiment_index', 50)
        
        # è®¡ç®—æ¯æ¡å¾®åšçš„åŠ æƒåˆ†æ•°
        weighted_scores = []
        
        for _, row in top_posts.iterrows():
            score_detail = self.calculate_weighted_sentiment(
                ai_score=ai_base_score,
                has_boost=row['å…³é”®è¯åŠ æˆ'],
                influence_weight=row['å½±å“åŠ›æƒé‡']
            )
            weighted_scores.append(score_detail['final_score'])
        
        # è®¡ç®—å¹³å‡åŠ æƒåˆ†æ•°
        avg_weighted_score = np.mean(weighted_scores)
        
        # 4. æ•´åˆç»“æœ
        ai_result['ai_base_score'] = ai_base_score
        ai_result['weighted_sentiment_index'] = round(avg_weighted_score, 2)
        ai_result['score_details'] = {
            'min_score': round(min(weighted_scores), 2),
            'max_score': round(max(weighted_scores), 2),
            'std_score': round(np.std(weighted_scores), 2)
        }
        
        print(f"âœ“ åŠ æƒè®¡ç®—å®Œæˆ")
        print(f"  AIåŸºç¡€åˆ†æ•°: {ai_base_score}")
        print(f"  åŠ æƒå¹³å‡åˆ†æ•°: {avg_weighted_score:.2f}")
        print(f"  åˆ†æ•°èŒƒå›´: {ai_result['score_details']['min_score']} - {ai_result['score_details']['max_score']}")
        
        return ai_result
    
    def _call_deepseek_api(self, text):
        """
        è°ƒç”¨DeepSeek API
        
        å‚æ•°:
            text: è¦åˆ†æçš„æ–‡æœ¬
            
        è¿”å›:
            dict: åˆ†æç»“æœ
        """
        url = f"{self.api_base}/chat/completions"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹å¾®åšå†…å®¹ï¼š\n\n{text}"}
            ],
            "temperature": 0.3,
            "max_tokens": 1000
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            
            if 'choices' in result and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content']
                
                try:
                    json_match = re.search(r'\{[\s\S]*\}', content)
                    if json_match:
                        json_str = json_match.group()
                        analysis_result = json.loads(json_str)
                        return analysis_result
                    else:
                        print("  è­¦å‘Š: æ— æ³•ä»å“åº”ä¸­æå–JSON")
                        return None
                except json.JSONDecodeError:
                    print(f"  è­¦å‘Š: JSONè§£æå¤±è´¥")
                    return None
            
            return None
            
        except Exception as e:
            print(f"  APIè°ƒç”¨å¤±è´¥: {e}")
            return None

    
    def generate_report(self, df, analysis_result):
        """
        ç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Šï¼ˆåŠ æƒç‰ˆæœ¬ï¼‰
        
        å‚æ•°:
            df: æ•°æ®DataFrame
            analysis_result: AIåˆ†æç»“æœ
            
        è¿”å›:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        print("\n" + "=" * 60)
        print("ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼ˆåŠ æƒä¼˜åŒ–ç‰ˆï¼‰")
        print("=" * 60)
        
        today = datetime.now().strftime('%Y%m%d')
        filename = f"å¾®åšé»„é‡‘æƒ…ç»ªåˆ†æ_åŠ æƒç‰ˆ_{today}.md"
        
        report_lines = []
        
        # æ ‡é¢˜
        report_lines.append(f"# å¾®åšé»„é‡‘æƒ…ç»ªåˆ†ææŠ¥å‘Šï¼ˆåŠ æƒä¼˜åŒ–ç‰ˆï¼‰")
        report_lines.append(f"")
        report_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        report_lines.append(f"")
        report_lines.append("---")
        report_lines.append("")
        
        # æ•°æ®æ¦‚å†µ
        report_lines.append("## ğŸ“Š æ•°æ®æ¦‚å†µ")
        report_lines.append("")
        report_lines.append(f"- **æŠ“å–å¾®åšæ•°**: {len(df)} æ¡")
        report_lines.append(f"- **å¹³å‡ç‚¹èµæ•°**: {df['ç‚¹èµæ•°'].mean():.0f}")
        report_lines.append(f"- **å¹³å‡è½¬å‘æ•°**: {df['è½¬å‘æ•°'].mean():.0f}")
        report_lines.append(f"- **æœ€é«˜ç‚¹èµ**: {df['ç‚¹èµæ•°'].max()}")
        report_lines.append("")
        
        # åŠ æƒç»Ÿè®¡
        report_lines.append("## âš–ï¸  åŠ æƒç»Ÿè®¡")
        report_lines.append("")
        
        high_influence = len(df[df['å½±å“åŠ›æƒé‡'] == 10])
        mid_influence = len(df[df['å½±å“åŠ›æƒé‡'] == 3])
        low_influence = len(df[df['å½±å“åŠ›æƒé‡'] == 1])
        
        report_lines.append("### åšä¸»å½±å“åŠ›åˆ†å¸ƒ")
        report_lines.append("")
        report_lines.append(f"- **é«˜å½±å“åŠ›åšä¸»** (100ä¸‡+ç²‰ä¸, æƒé‡Ã—10): {high_influence} æ¡")
        report_lines.append(f"- **ä¸­å½±å“åŠ›åšä¸»** (10ä¸‡+ç²‰ä¸, æƒé‡Ã—3): {mid_influence} æ¡")
        report_lines.append(f"- **æ™®é€šåšä¸»** (æƒé‡Ã—1): {low_influence} æ¡")
        report_lines.append("")
        
        boost_count = len(df[df['å…³é”®è¯åŠ æˆ'] == True])
        report_lines.append("### å…³é”®è¯åŠ æˆ")
        report_lines.append("")
        report_lines.append(f"- **åŒ…å«å…³é”®è¯åŠ æˆ**: {boost_count} æ¡ (+20%)")
        report_lines.append(f"- **å…³é”®è¯**: {', '.join(self.boost_keywords)}")
        report_lines.append("")
        
        # AIåˆ†æç»“æœ
        if analysis_result:
            report_lines.append("## ğŸ¤– AIæƒ…ç»ªåˆ†æï¼ˆåŠ æƒä¼˜åŒ–ï¼‰")
            report_lines.append("")
            
            # åŸºç¡€åˆ†æ•° vs åŠ æƒåˆ†æ•°
            ai_base_score = analysis_result.get('ai_base_score', 50)
            weighted_score = analysis_result.get('weighted_sentiment_index', 50)
            sentiment_label = analysis_result.get('sentiment_label', 'ä¸­æ€§')
            
            report_lines.append(f"### æƒ…ç»ªæŒ‡æ•°å¯¹æ¯”")
            report_lines.append("")
            report_lines.append(f"| æŒ‡æ ‡ | åˆ†æ•° | è¯´æ˜ |")
            report_lines.append(f"|------|------|------|")
            report_lines.append(f"| AIåŸºç¡€åˆ†æ•° | {ai_base_score} / 100 | DeepSeekåŸå§‹åˆ†æ |")
            report_lines.append(f"| åŠ æƒå¹³å‡åˆ†æ•° | {weighted_score} / 100 | è€ƒè™‘å½±å“åŠ›+å…³é”®è¯ |")
            report_lines.append(f"| æƒ…ç»ªæ ‡ç­¾ | {sentiment_label} | - |")
            report_lines.append("")
            
            # åˆ†æ•°è¯¦æƒ…
            score_details = analysis_result.get('score_details', {})
            report_lines.append(f"**åˆ†æ•°ç»Ÿè®¡**:")
            report_lines.append(f"- æœ€ä½åˆ†: {score_details.get('min_score', 0)}")
            report_lines.append(f"- æœ€é«˜åˆ†: {score_details.get('max_score', 0)}")
            report_lines.append(f"- æ ‡å‡†å·®: {score_details.get('std_score', 0)}")
            report_lines.append("")
            
            # æƒ…ç»ªæ¡å½¢å›¾
            bar_length = int(weighted_score / 5)
            bar = "ğŸŸ©" * bar_length + "â¬œ" * (20 - bar_length)
            report_lines.append(f"```")
            report_lines.append(f"{bar}")
            report_lines.append(f"0    20   40   60   80   100")
            report_lines.append(f"```")
            report_lines.append("")
            
            # ä¸€å¥è¯æ€»ç»“
            summary = analysis_result.get('summary', '')
            if summary:
                report_lines.append(f"**ä»Šæ—¥æƒ…ç»ª**: {summary}")
                report_lines.append("")
            
            # é£é™©ç‚¹
            risk_points = analysis_result.get('risk_points', [])
            if risk_points:
                report_lines.append("### âš ï¸  ç”¨æˆ·æœ€æ‹…å¿ƒçš„3ä¸ªé£é™©ç‚¹")
                report_lines.append("")
                for i, risk in enumerate(risk_points, 1):
                    report_lines.append(f"{i}. {risk}")
                report_lines.append("")
            
            # æœºä¼šç‚¹
            opportunity_points = analysis_result.get('opportunity_points', [])
            if opportunity_points:
                report_lines.append("### ğŸ’¡ ç”¨æˆ·æœ€æœŸå¾…çš„3ä¸ªæœºä¼šç‚¹")
                report_lines.append("")
                for i, opp in enumerate(opportunity_points, 1):
                    report_lines.append(f"{i}. {opp}")
                report_lines.append("")
        
        # é«˜å½±å“åŠ›åšä¸»TOP 5
        report_lines.append("## ğŸ‘‘ é«˜å½±å“åŠ›åšä¸» TOP 5")
        report_lines.append("")
        
        top_influencers = df.nlargest(5, 'ç²‰ä¸æ•°')
        for i, (_, row) in enumerate(top_influencers.iterrows(), 1):
            report_lines.append(f"### {i}. @{row['åšä¸»å']}")
            report_lines.append(f"")
            report_lines.append(f"- ğŸ‘¥ ç²‰ä¸æ•°: {row['ç²‰ä¸æ•°']:,}")
            report_lines.append(f"- âš–ï¸  å½±å“åŠ›æƒé‡: Ã—{row['å½±å“åŠ›æƒé‡']}")
            report_lines.append(f"- ğŸ”‘ å…³é”®è¯åŠ æˆ: {'æ˜¯' if row['å…³é”®è¯åŠ æˆ'] else 'å¦'}")
            if row['åŒ¹é…å…³é”®è¯']:
                report_lines.append(f"- ğŸ“Œ åŒ¹é…å…³é”®è¯: {row['åŒ¹é…å…³é”®è¯']}")
            report_lines.append(f"")
            report_lines.append(f"> {row['åšæ–‡å†…å®¹'][:150]}{'...' if len(row['åšæ–‡å†…å®¹']) > 150 else ''}")
            report_lines.append(f"")
            report_lines.append(f"- ğŸ‘ ç‚¹èµ: {row['ç‚¹èµæ•°']} | ğŸ”„ è½¬å‘: {row['è½¬å‘æ•°']} | ğŸ“… {row['å‘å¸ƒæ—¶é—´']}")
            report_lines.append(f"")
        
        # å…³é”®è¯åŠ æˆå¾®åš
        boost_posts = df[df['å…³é”®è¯åŠ æˆ'] == True]
        if not boost_posts.empty:
            report_lines.append("## ğŸ”¥ åŒ…å«å…³é”®è¯åŠ æˆçš„å¾®åš")
            report_lines.append("")
            
            for i, (_, row) in enumerate(boost_posts.head(5).iterrows(), 1):
                report_lines.append(f"### {i}. @{row['åšä¸»å']}")
                report_lines.append(f"")
                report_lines.append(f"- ğŸ“Œ åŒ¹é…å…³é”®è¯: {row['åŒ¹é…å…³é”®è¯']}")
                report_lines.append(f"- âš–ï¸  å½±å“åŠ›æƒé‡: Ã—{row['å½±å“åŠ›æƒé‡']}")
                report_lines.append(f"")
                report_lines.append(f"> {row['åšæ–‡å†…å®¹'][:150]}{'...' if len(row['åšæ–‡å†…å®¹']) > 150 else ''}")
                report_lines.append(f"")
                report_lines.append(f"- ğŸ‘ ç‚¹èµ: {row['ç‚¹èµæ•°']} | ğŸ”„ è½¬å‘: {row['è½¬å‘æ•°']}")
                report_lines.append(f"")
        
        # åŠ æƒå…¬å¼è¯´æ˜
        report_lines.append("---")
        report_lines.append("")
        report_lines.append("## ğŸ“ åŠ æƒå…¬å¼è¯´æ˜")
        report_lines.append("")
        report_lines.append("### å½±å“åŠ›æƒé‡")
        report_lines.append("```")
        report_lines.append("ç²‰ä¸æ•° >= 100ä¸‡: æƒé‡ = 10")
        report_lines.append("ç²‰ä¸æ•° >= 10ä¸‡:  æƒé‡ = 3")
        report_lines.append("å…¶ä»–:            æƒé‡ = 1")
        report_lines.append("```")
        report_lines.append("")
        report_lines.append("### å…³é”®è¯åŠ æˆ")
        report_lines.append("```")
        report_lines.append(f"å…³é”®è¯: {', '.join(self.boost_keywords)}")
        report_lines.append(f"åŠ æˆæ¯”ä¾‹: +{self.boost_ratio * 100}%")
        report_lines.append("```")
        report_lines.append("")
        report_lines.append("### åŠ æƒå…¬å¼")
        report_lines.append("```")
        report_lines.append("Final_Score = (AI_Sentiment_Score + Keyword_Bonus) Ã— Influence_Weight")
        report_lines.append("")
        report_lines.append("å…¶ä¸­:")
        report_lines.append("- AI_Sentiment_Score: DeepSeek AIåˆ†æçš„åŸºç¡€åˆ†æ•° (0-100)")
        report_lines.append("- Keyword_Bonus: AIåˆ†æ•° Ã— 20% (å¦‚æœåŒ…å«å…³é”®è¯)")
        report_lines.append("- Influence_Weight: åšä¸»å½±å“åŠ›æƒé‡ (1, 3, æˆ– 10)")
        report_lines.append("")
        report_lines.append("å½’ä¸€åŒ–:")
        report_lines.append("- æœ€å¤§å¯èƒ½å€¼: (100 + 20) Ã— 10 = 1200")
        report_lines.append("- å½’ä¸€åŒ–å…¬å¼: (Final_Score / 1200) Ã— 100")
        report_lines.append("- ç¡®ä¿æœ€ç»ˆåˆ†æ•°åœ¨ 0-100 ä¹‹é—´")
        report_lines.append("```")
        report_lines.append("")
        
        # å…è´£å£°æ˜
        report_lines.append("---")
        report_lines.append("")
        report_lines.append("## âš ï¸  å…è´£å£°æ˜")
        report_lines.append("")
        report_lines.append("æœ¬æŠ¥å‘ŠåŸºäºå¾®åšå…¬å¼€æ•°æ®å’ŒAIåˆ†æç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚")
        report_lines.append("ç¤¾äº¤åª’ä½“æƒ…ç»ªå…·æœ‰æ³¢åŠ¨æ€§ï¼Œè¯·ç»“åˆå…¶ä»–ä¿¡æ¯æºç»¼åˆåˆ¤æ–­ã€‚")
        report_lines.append("")
        
        # å†™å…¥æ–‡ä»¶
        report_content = "\n".join(report_lines)
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {filename}")
        
        return filename
    
    def run(self, pages=3, headless=False):
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹ï¼ˆåŠ æƒä¼˜åŒ–ç‰ˆï¼‰
        
        å‚æ•°:
            pages: æŠ“å–é¡µæ•°
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
        """
        print("=" * 60)
        print("å¾®åšé»„é‡‘æƒ…ç»ªè‡ªåŠ¨åˆ†æç³»ç»Ÿï¼ˆåŠ æƒä¼˜åŒ–ç‰ˆï¼‰")
        print("=" * 60)
        print()
        
        # 1. æŠ“å–æ•°æ®
        df = self.scrape_weibo_gold(pages=pages, headless=headless)
        
        if df.empty:
            print("\nÃ— æœªæŠ“å–åˆ°æ•°æ®ï¼Œç¨‹åºç»“æŸ")
            return
        
        # ä¿å­˜åŸå§‹æ•°æ®
        raw_filename = f"weibo_raw_weighted_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(raw_filename, index=False, encoding='utf-8-sig')
        print(f"\nåŸå§‹æ•°æ®å·²ä¿å­˜: {raw_filename}")
        
        # 2. æ¸…æ´—æ•°æ®
        df_clean = self.clean_data(df)
        
        if df_clean.empty:
            print("\nÃ— æ¸…æ´—åæ— æœ‰æ•ˆæ•°æ®ï¼Œç¨‹åºç»“æŸ")
            return
        
        # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        clean_filename = f"weibo_clean_weighted_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df_clean.to_csv(clean_filename, index=False, encoding='utf-8-sig')
        print(f"æ¸…æ´—æ•°æ®å·²ä¿å­˜: {clean_filename}")
        
        # 3. AIåŠ æƒåˆ†æ
        analysis_result = self.analyze_sentiment_with_ai_weighted(df_clean)
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        report_file = self.generate_report(df_clean, analysis_result)
        
        print("\n" + "=" * 60)
        print("âœ… åˆ†æå®Œæˆï¼")
        print("=" * 60)
        print(f"\nç”Ÿæˆæ–‡ä»¶:")
        print(f"  - åŸå§‹æ•°æ®: {raw_filename}")
        print(f"  - æ¸…æ´—æ•°æ®: {clean_filename}")
        print(f"  - åˆ†ææŠ¥å‘Š: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = WeiboSentimentWeightedAnalyzer()
    
    # è¿è¡Œåˆ†æ
    analyzer.run(pages=3, headless=False)


if __name__ == "__main__":
    main()
