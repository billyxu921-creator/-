#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®åšé»„é‡‘æƒ…ç»ªè‡ªåŠ¨åˆ†æç³»ç»Ÿ
ä½¿ç”¨PlaywrightæŠ“å–å¾®åšæ•°æ®ï¼ŒDeepSeek AIåˆ†ææƒ…ç»ª
"""

import pandas as pd
import random
import time
import json
import requests
from datetime import datetime
from playwright.sync_api import sync_playwright
import re


class WeiboGoldSentimentAnalyzer:
    """å¾®åšé»„é‡‘æƒ…ç»ªåˆ†æå™¨"""
    
    def __init__(self, api_key=None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        å‚æ•°:
            api_key: DeepSeek APIå¯†é’¥
        """
        # ä»é…ç½®æ–‡ä»¶è¯»å–API Key
        if api_key is None:
            try:
                from config import DEEPSEEK_CONFIG
                self.api_key = DEEPSEEK_CONFIG['api_key']
                self.api_base = DEEPSEEK_CONFIG['api_base']
            except ImportError:
                print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œè¯·è®¾ç½®API Key")
                self.api_key = "YOUR_API_KEY"
                self.api_base = "https://api.deepseek.com/v1"
        else:
            self.api_key = api_key
            self.api_base = "https://api.deepseek.com/v1"
        
        # è¥é”€å¹¿å‘Šå…³é”®è¯ï¼ˆç”¨äºè¿‡æ»¤ï¼‰
        self.spam_keywords = [
            'æŠ½å¥–', 'è½¬è¿ç ', 'ä»£è´­', 'å¾®å•†', 'åŠ å¾®ä¿¡', 'æ‰«ç ',
            'ä¼˜æƒ ', 'ä¿ƒé”€', 'æ‰“æŠ˜', 'ç§’æ€', 'æ‹¼å›¢', 'ç ä»·',
            'å…è´¹é¢†', 'é™æ—¶', 'ç‰¹ä»·', 'åŒ…é‚®', 'ç›´æ’­é—´'
        ]
        
        # AIåˆ†æçš„ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = """ä½ æ˜¯ä¸€ä½è¡Œä¸ºé‡‘èå­¦ä¸“å®¶ï¼Œè¯·åˆ†æè¿™äº›å¾®åšæ–‡æœ¬ï¼Œç»™å‡ºä»¥ä¸‹å†…å®¹ï¼š

1. é»„é‡‘çœ‹æ¶¨æƒ…ç»ªæŒ‡æ•°ï¼ˆ0-100åˆ†ï¼‰
   - 0-20: æåº¦æ‚²è§‚ï¼Œææ…Œæ€§æŠ›å”®
   - 21-40: æ‚²è§‚ï¼Œçœ‹è·Œæƒ…ç»ªæ˜æ˜¾
   - 41-60: ä¸­æ€§ï¼Œè§‚æœ›ä¸ºä¸»
   - 61-80: ä¹è§‚ï¼Œçœ‹æ¶¨æƒ…ç»ªæ˜æ˜¾
   - 81-100: æåº¦ä¹è§‚ï¼Œè¿½æ¶¨çƒ­æƒ…é«˜

2. ä»Šæ—¥å¾®åšç”¨æˆ·æœ€æ‹…å¿ƒçš„3ä¸ªé£é™©ç‚¹

3. ä»Šæ—¥å¾®åšç”¨æˆ·æœ€æœŸå¾…çš„3ä¸ªæœºä¼šç‚¹

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
{
    "sentiment_index": æ•°å€¼(0-100),
    "sentiment_label": "æƒ…ç»ªæ ‡ç­¾",
    "risk_points": ["é£é™©ç‚¹1", "é£é™©ç‚¹2", "é£é™©ç‚¹3"],
    "opportunity_points": ["æœºä¼šç‚¹1", "æœºä¼šç‚¹2", "æœºä¼šç‚¹3"],
    "summary": "ä¸€å¥è¯æ€»ç»“ä»Šæ—¥é»„é‡‘æƒ…ç»ª"
}"""
    
    def scrape_weibo_gold(self, pages=3, headless=False):
        """
        ä½¿ç”¨PlaywrightæŠ“å–å¾®åšé»„é‡‘ç›¸å…³å†…å®¹
        
        å‚æ•°:
            pages: æŠ“å–é¡µæ•°ï¼ˆé»˜è®¤3é¡µï¼‰
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆFalseæ–¹ä¾¿æ‰«ç ç™»å½•ï¼‰
            
        è¿”å›:
            DataFrame: åŒ…å«å¾®åšæ•°æ®
        """
        print("=" * 60)
        print("å¼€å§‹æŠ“å–å¾®åšé»„é‡‘ç›¸å…³å†…å®¹")
        print("=" * 60)
        
        all_posts = []
        
        with sync_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨ï¼ˆheadless=Falseæ–¹ä¾¿æ‰«ç ç™»å½•ï¼‰
            print(f"å¯åŠ¨æµè§ˆå™¨ (headless={headless})...")
            browser = p.chromium.launch(headless=headless)
            
            # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼ˆæ¨¡æ‹ŸçœŸå®ç”¨æˆ·ï¼‰
            context = browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            )
            
            # åˆ›å»ºæ–°é¡µé¢
            page = context.new_page()
            
            try:
                # 1. å¯¼èˆªåˆ°å¾®åšæœç´¢é¡µ
                print("\næ­¥éª¤1: è®¿é—®å¾®åšæœç´¢é¡µ...")
                weibo_search_url = "https://s.weibo.com/weibo?q=é»„é‡‘"
                page.goto(weibo_search_url, timeout=30000)
                
                # éšæœºç­‰å¾…ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·
                wait_time = random.uniform(3, 7)
                print(f"ç­‰å¾… {wait_time:.1f} ç§’...")
                time.sleep(wait_time)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
                if "login" in page.url.lower() or page.locator('text=ç™»å½•').count() > 0:
                    print("\nâš ï¸  éœ€è¦ç™»å½•å¾®åš")
                    print("è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼ˆæ‰«ç æˆ–è´¦å·å¯†ç ï¼‰")
                    print("ç™»å½•å®Œæˆåï¼ŒæŒ‰å›è½¦ç»§ç»­...")
                    input()
                    
                    # ç™»å½•åé‡æ–°è®¿é—®æœç´¢é¡µ
                    page.goto(weibo_search_url, timeout=30000)
                    time.sleep(random.uniform(3, 5))
                
                # 2. å¼€å§‹æŠ“å–æ•°æ®
                print(f"\næ­¥éª¤2: å¼€å§‹æŠ“å–æ•°æ®ï¼ˆå…±{pages}é¡µï¼‰...")
                
                for page_num in range(1, pages + 1):
                    print(f"\næ­£åœ¨æŠ“å–ç¬¬ {page_num}/{pages} é¡µ...")
                    
                    # ç­‰å¾…å†…å®¹åŠ è½½
                    try:
                        page.wait_for_selector('.card-wrap', timeout=10000)
                    except:
                        print("  âš ï¸  é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­...")
                    
                    # æ»šåŠ¨é¡µé¢ï¼Œè§¦å‘æ‡’åŠ è½½
                    print("  æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹...")
                    for scroll in range(3):
                        page.evaluate('window.scrollBy(0, 800)')
                        time.sleep(random.uniform(1, 2))
                    
                    # æå–å½“å‰é¡µé¢çš„å¾®åšå†…å®¹
                    posts = self._extract_posts_from_page(page)
                    all_posts.extend(posts)
                    print(f"  âœ“ æœ¬é¡µæå– {len(posts)} æ¡å¾®åš")
                    
                    # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼Œç‚¹å‡»ä¸‹ä¸€é¡µ
                    if page_num < pages:
                        try:
                            # æŸ¥æ‰¾å¹¶ç‚¹å‡»"ä¸‹ä¸€é¡µ"æŒ‰é’®
                            next_button = page.locator('a.next')
                            if next_button.count() > 0:
                                print("  ç‚¹å‡»ä¸‹ä¸€é¡µ...")
                                next_button.click()
                                
                                # éšæœºç­‰å¾…ï¼Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
                                wait_time = random.uniform(4, 8)
                                print(f"  ç­‰å¾… {wait_time:.1f} ç§’...")
                                time.sleep(wait_time)
                            else:
                                print("  æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œåœæ­¢æŠ“å–")
                                break
                        except Exception as e:
                            print(f"  ç¿»é¡µå¤±è´¥: {e}")
                            break
                
                print(f"\nâœ“ æŠ“å–å®Œæˆï¼Œå…±è·å– {len(all_posts)} æ¡å¾®åš")
                
            except Exception as e:
                print(f"\nÃ— æŠ“å–è¿‡ç¨‹å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
            
            finally:
                # å…³é—­æµè§ˆå™¨
                print("\nå…³é—­æµè§ˆå™¨...")
                browser.close()
        
        # è½¬æ¢ä¸ºDataFrame
        if all_posts:
            df = pd.DataFrame(all_posts)
            return df
        else:
            return pd.DataFrame()
    
    def _extract_posts_from_page(self, page):
        """
        ä»å½“å‰é¡µé¢æå–å¾®åšå†…å®¹
        
        å‚æ•°:
            page: Playwrighté¡µé¢å¯¹è±¡
            
        è¿”å›:
            list: å¾®åšæ•°æ®åˆ—è¡¨
        """
        posts = []
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰å¾®åšå¡ç‰‡
            cards = page.locator('.card-wrap').all()
            
            for card in cards:
                try:
                    post_data = {}
                    
                    # æå–åšä¸»å
                    try:
                        author = card.locator('.name').inner_text()
                        post_data['åšä¸»å'] = author.strip()
                    except:
                        post_data['åšä¸»å'] = 'æœªçŸ¥'
                    
                    # æå–åšæ–‡å†…å®¹
                    try:
                        content = card.locator('.txt').inner_text()
                        post_data['åšæ–‡å†…å®¹'] = content.strip()
                    except:
                        post_data['åšæ–‡å†…å®¹'] = ''
                    
                    # æå–å‘å¸ƒæ—¶é—´
                    try:
                        pub_time = card.locator('.from').inner_text()
                        post_data['å‘å¸ƒæ—¶é—´'] = pub_time.strip()
                    except:
                        post_data['å‘å¸ƒæ—¶é—´'] = ''
                    
                    # æå–ç‚¹èµæ•°
                    try:
                        likes = card.locator('text=/èµ/').inner_text()
                        # æå–æ•°å­—
                        like_num = re.findall(r'\d+', likes)
                        post_data['ç‚¹èµæ•°'] = int(like_num[0]) if like_num else 0
                    except:
                        post_data['ç‚¹èµæ•°'] = 0
                    
                    # æå–è½¬å‘æ•°
                    try:
                        retweets = card.locator('text=/è½¬å‘/').inner_text()
                        retweet_num = re.findall(r'\d+', retweets)
                        post_data['è½¬å‘æ•°'] = int(retweet_num[0]) if retweet_num else 0
                    except:
                        post_data['è½¬å‘æ•°'] = 0
                    
                    # åªä¿å­˜æœ‰å†…å®¹çš„å¾®åš
                    if post_data['åšæ–‡å†…å®¹']:
                        posts.append(post_data)
                
                except Exception as e:
                    # å•æ¡å¾®åšæå–å¤±è´¥ä¸å½±å“å…¶ä»–
                    continue
        
        except Exception as e:
            print(f"    æå–å¾®åšå¤±è´¥: {e}")
        
        return posts
    
    def clean_data(self, df):
        """
        æ¸…æ´—æ•°æ®
        
        å‚æ•°:
            df: åŸå§‹æ•°æ®DataFrame
            
        è¿”å›:
            DataFrame: æ¸…æ´—åçš„æ•°æ®
        """
        print("\n" + "=" * 60)
        print("å¼€å§‹æ•°æ®æ¸…æ´—")
        print("=" * 60)
        
        original_count = len(df)
        print(f"åŸå§‹æ•°æ®: {original_count} æ¡")
        
        # 1. è¿‡æ»¤è¥é”€å¹¿å‘Š
        print("\næ­¥éª¤1: è¿‡æ»¤è¥é”€å¹¿å‘Š...")
        mask = df['åšæ–‡å†…å®¹'].apply(lambda x: not any(keyword in str(x) for keyword in self.spam_keywords))
        df = df[mask].copy()
        print(f"  è¿‡æ»¤å: {len(df)} æ¡ (ç§»é™¤ {original_count - len(df)} æ¡)")
        
        # 2. å»é™¤è¿‡çŸ­çš„å†…å®¹ï¼ˆå°‘äº10ä¸ªå­—ï¼‰
        print("\næ­¥éª¤2: è¿‡æ»¤è¿‡çŸ­å†…å®¹...")
        before = len(df)
        df = df[df['åšæ–‡å†…å®¹'].str.len() >= 10].copy()
        print(f"  è¿‡æ»¤å: {len(df)} æ¡ (ç§»é™¤ {before - len(df)} æ¡)")
        
        # 3. å»é‡ï¼ˆæ ¹æ®åšæ–‡å†…å®¹ï¼‰
        print("\næ­¥éª¤3: å»é™¤é‡å¤å†…å®¹...")
        before = len(df)
        df = df.drop_duplicates(subset=['åšæ–‡å†…å®¹'], keep='first')
        print(f"  å»é‡å: {len(df)} æ¡ (ç§»é™¤ {before - len(df)} æ¡)")
        
        # 4. é‡ç½®ç´¢å¼•
        df = df.reset_index(drop=True)
        
        print(f"\nâœ“ æ¸…æ´—å®Œæˆï¼Œæœ€ç»ˆä¿ç•™ {len(df)} æ¡æœ‰æ•ˆæ•°æ®")
        
        return df
    
    def analyze_sentiment_with_ai(self, df):
        """
        ä½¿ç”¨DeepSeek AIåˆ†ææƒ…ç»ª
        
        å‚æ•°:
            df: æ¸…æ´—åçš„æ•°æ®DataFrame
            
        è¿”å›:
            dict: AIåˆ†æç»“æœ
        """
        print("\n" + "=" * 60)
        print("å¼€å§‹AIæƒ…ç»ªåˆ†æ")
        print("=" * 60)
        
        if df.empty:
            print("Ã— æ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æ")
            return None
        
        # 1. åˆå¹¶æ‰€æœ‰åšæ–‡å†…å®¹
        print(f"\nå‡†å¤‡åˆ†æ {len(df)} æ¡å¾®åš...")
        
        # é€‰æ‹©ç‚¹èµæ•°å’Œè½¬å‘æ•°è¾ƒé«˜çš„å¾®åšï¼ˆæ›´æœ‰ä»£è¡¨æ€§ï¼‰
        df_sorted = df.sort_values(by=['ç‚¹èµæ•°', 'è½¬å‘æ•°'], ascending=False)
        
        # å–å‰50æ¡æˆ–å…¨éƒ¨ï¼ˆå¦‚æœå°‘äº50æ¡ï¼‰
        top_posts = df_sorted.head(50)
        
        # åˆå¹¶å†…å®¹
        combined_text = "\n\n---\n\n".join([
            f"ã€å¾®åš{i+1}ã€‘{row['åšæ–‡å†…å®¹']}"
            for i, (_, row) in enumerate(top_posts.iterrows())
        ])
        
        print(f"é€‰å– {len(top_posts)} æ¡ä»£è¡¨æ€§å¾®åšè¿›è¡Œåˆ†æ")
        print(f"æ€»å­—æ•°: {len(combined_text)} å­—")
        
        # 2. è°ƒç”¨DeepSeek API
        print("\næ­£åœ¨è°ƒç”¨DeepSeek API...")
        
        result = self._call_deepseek_api(combined_text)
        
        if result:
            print("âœ“ AIåˆ†æå®Œæˆ")
            return result
        else:
            print("Ã— AIåˆ†æå¤±è´¥")
            return None
    
    def _call_deepseek_api(self, text):
        """
        è°ƒç”¨DeepSeek API
        
        å‚æ•°:
            text: è¦åˆ†æçš„æ–‡æœ¬
            
        è¿”å›:
            dict: åˆ†æç»“æœ
        """
        url = f"{self.api_base}/chat/completions"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹å¾®åšå†…å®¹ï¼š\n\n{text}"}
            ],
            "temperature": 0.3,
            "max_tokens": 1000
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            
            if 'choices' in result and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content']
                
                # å°è¯•è§£æJSON
                try:
                    # æå–JSONéƒ¨åˆ†ï¼ˆå¯èƒ½åŒ…å«åœ¨markdownä»£ç å—ä¸­ï¼‰
                    json_match = re.search(r'\{[\s\S]*\}', content)
                    if json_match:
                        json_str = json_match.group()
                        analysis_result = json.loads(json_str)
                        return analysis_result
                    else:
                        print("  è­¦å‘Š: æ— æ³•ä»å“åº”ä¸­æå–JSON")
                        return None
                except json.JSONDecodeError:
                    print(f"  è­¦å‘Š: JSONè§£æå¤±è´¥")
                    print(f"  åŸå§‹å“åº”: {content[:200]}...")
                    return None
            
            return None
            
        except Exception as e:
            print(f"  APIè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def generate_report(self, df, analysis_result):
        """
        ç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š
        
        å‚æ•°:
            df: æ•°æ®DataFrame
            analysis_result: AIåˆ†æç»“æœ
            
        è¿”å›:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        print("\n" + "=" * 60)
        print("ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        print("=" * 60)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆä»¥æ—¥æœŸå‘½åï¼‰
        today = datetime.now().strftime('%Y%m%d')
        filename = f"å¾®åšé»„é‡‘æƒ…ç»ªåˆ†æ_{today}.md"
        
        # æ„å»ºæŠ¥å‘Šå†…å®¹
        report_lines = []
        
        # æ ‡é¢˜
        report_lines.append(f"# å¾®åšé»„é‡‘æƒ…ç»ªåˆ†ææŠ¥å‘Š")
        report_lines.append(f"")
        report_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        report_lines.append(f"")
        report_lines.append("---")
        report_lines.append("")
        
        # æ•°æ®æ¦‚å†µ
        report_lines.append("## ğŸ“Š æ•°æ®æ¦‚å†µ")
        report_lines.append("")
        report_lines.append(f"- **æŠ“å–å¾®åšæ•°**: {len(df)} æ¡")
        report_lines.append(f"- **å¹³å‡ç‚¹èµæ•°**: {df['ç‚¹èµæ•°'].mean():.0f}")
        report_lines.append(f"- **å¹³å‡è½¬å‘æ•°**: {df['è½¬å‘æ•°'].mean():.0f}")
        report_lines.append(f"- **æœ€é«˜ç‚¹èµ**: {df['ç‚¹èµæ•°'].max()}")
        report_lines.append("")
        
        # AIåˆ†æç»“æœ
        if analysis_result:
            report_lines.append("## ğŸ¤– AIæƒ…ç»ªåˆ†æ")
            report_lines.append("")
            
            # æƒ…ç»ªæŒ‡æ•°
            sentiment_index = analysis_result.get('sentiment_index', 50)
            sentiment_label = analysis_result.get('sentiment_label', 'ä¸­æ€§')
            
            report_lines.append(f"### é»„é‡‘çœ‹æ¶¨æƒ…ç»ªæŒ‡æ•°")
            report_lines.append("")
            report_lines.append(f"**{sentiment_index} / 100** - {sentiment_label}")
            report_lines.append("")
            
            # æƒ…ç»ªæ¡å½¢å›¾ï¼ˆç”¨emojiè¡¨ç¤ºï¼‰
            bar_length = int(sentiment_index / 5)
            bar = "ğŸŸ©" * bar_length + "â¬œ" * (20 - bar_length)
            report_lines.append(f"```")
            report_lines.append(f"{bar}")
            report_lines.append(f"0    20   40   60   80   100")
            report_lines.append(f"```")
            report_lines.append("")
            
            # ä¸€å¥è¯æ€»ç»“
            summary = analysis_result.get('summary', '')
            if summary:
                report_lines.append(f"**ä»Šæ—¥æƒ…ç»ª**: {summary}")
                report_lines.append("")
            
            # é£é™©ç‚¹
            risk_points = analysis_result.get('risk_points', [])
            if risk_points:
                report_lines.append("### âš ï¸  ç”¨æˆ·æœ€æ‹…å¿ƒçš„3ä¸ªé£é™©ç‚¹")
                report_lines.append("")
                for i, risk in enumerate(risk_points, 1):
                    report_lines.append(f"{i}. {risk}")
                report_lines.append("")
            
            # æœºä¼šç‚¹
            opportunity_points = analysis_result.get('opportunity_points', [])
            if opportunity_points:
                report_lines.append("### ğŸ’¡ ç”¨æˆ·æœ€æœŸå¾…çš„3ä¸ªæœºä¼šç‚¹")
                report_lines.append("")
                for i, opp in enumerate(opportunity_points, 1):
                    report_lines.append(f"{i}. {opp}")
                report_lines.append("")
        
        # çƒ­é—¨å¾®åš
        report_lines.append("## ğŸ”¥ çƒ­é—¨å¾®åš TOP 10")
        report_lines.append("")
        
        top_posts = df.nlargest(10, 'ç‚¹èµæ•°')
        for i, (_, row) in enumerate(top_posts.iterrows(), 1):
            report_lines.append(f"### {i}. @{row['åšä¸»å']}")
            report_lines.append(f"")
            report_lines.append(f"> {row['åšæ–‡å†…å®¹'][:200]}{'...' if len(row['åšæ–‡å†…å®¹']) > 200 else ''}")
            report_lines.append(f"")
            report_lines.append(f"- ğŸ‘ ç‚¹èµ: {row['ç‚¹èµæ•°']} | ğŸ”„ è½¬å‘: {row['è½¬å‘æ•°']} | ğŸ“… {row['å‘å¸ƒæ—¶é—´']}")
            report_lines.append(f"")
        
        # å…è´£å£°æ˜
        report_lines.append("---")
        report_lines.append("")
        report_lines.append("## âš ï¸  å…è´£å£°æ˜")
        report_lines.append("")
        report_lines.append("æœ¬æŠ¥å‘ŠåŸºäºå¾®åšå…¬å¼€æ•°æ®å’ŒAIåˆ†æç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚")
        report_lines.append("ç¤¾äº¤åª’ä½“æƒ…ç»ªå…·æœ‰æ³¢åŠ¨æ€§ï¼Œè¯·ç»“åˆå…¶ä»–ä¿¡æ¯æºç»¼åˆåˆ¤æ–­ã€‚")
        report_lines.append("")
        
        # å†™å…¥æ–‡ä»¶
        report_content = "\n".join(report_lines)
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {filename}")
        
        return filename
    
    def run(self, pages=3, headless=False):
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        
        å‚æ•°:
            pages: æŠ“å–é¡µæ•°
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
        """
        print("=" * 60)
        print("å¾®åšé»„é‡‘æƒ…ç»ªè‡ªåŠ¨åˆ†æç³»ç»Ÿ")
        print("=" * 60)
        print()
        
        # 1. æŠ“å–æ•°æ®
        df = self.scrape_weibo_gold(pages=pages, headless=headless)
        
        if df.empty:
            print("\nÃ— æœªæŠ“å–åˆ°æ•°æ®ï¼Œç¨‹åºç»“æŸ")
            return
        
        # ä¿å­˜åŸå§‹æ•°æ®
        raw_filename = f"weibo_raw_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(raw_filename, index=False, encoding='utf-8-sig')
        print(f"\nåŸå§‹æ•°æ®å·²ä¿å­˜: {raw_filename}")
        
        # 2. æ¸…æ´—æ•°æ®
        df_clean = self.clean_data(df)
        
        if df_clean.empty:
            print("\nÃ— æ¸…æ´—åæ— æœ‰æ•ˆæ•°æ®ï¼Œç¨‹åºç»“æŸ")
            return
        
        # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        clean_filename = f"weibo_clean_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df_clean.to_csv(clean_filename, index=False, encoding='utf-8-sig')
        print(f"æ¸…æ´—æ•°æ®å·²ä¿å­˜: {clean_filename}")
        
        # 3. AIåˆ†æ
        analysis_result = self.analyze_sentiment_with_ai(df_clean)
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        report_file = self.generate_report(df_clean, analysis_result)
        
        print("\n" + "=" * 60)
        print("âœ… åˆ†æå®Œæˆï¼")
        print("=" * 60)
        print(f"\nç”Ÿæˆæ–‡ä»¶:")
        print(f"  - åŸå§‹æ•°æ®: {raw_filename}")
        print(f"  - æ¸…æ´—æ•°æ®: {clean_filename}")
        print(f"  - åˆ†ææŠ¥å‘Š: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = WeiboGoldSentimentAnalyzer()
    
    # è¿è¡Œåˆ†æ
    # pages=3: æŠ“å–3é¡µæ•°æ®
    # headless=False: æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼Œæ–¹ä¾¿æ‰«ç ç™»å½•
    analyzer.run(pages=3, headless=False)


if __name__ == "__main__":
    main()